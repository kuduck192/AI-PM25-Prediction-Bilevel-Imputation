{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42592cb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T17:21:15.581267Z",
     "iopub.status.busy": "2025-04-20T17:21:15.580931Z",
     "iopub.status.idle": "2025-04-20T17:21:44.012031Z",
     "shell.execute_reply": "2025-04-20T17:21:44.011117Z"
    },
    "papermill": {
     "duration": 28.439554,
     "end_time": "2025-04-20T17:21:44.013887",
     "exception": false,
     "start_time": "2025-04-20T17:21:15.574333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import sys\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def manual_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # if you are suing GPU\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "manual_seed(SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f28f538",
   "metadata": {
    "papermill": {
     "duration": 0.004497,
     "end_time": "2025-04-20T17:21:44.023482",
     "exception": false,
     "start_time": "2025-04-20T17:21:44.018985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Data & Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0fb939",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T17:21:44.034177Z",
     "iopub.status.busy": "2025-04-20T17:21:44.033581Z",
     "iopub.status.idle": "2025-04-20T17:21:44.173433Z",
     "shell.execute_reply": "2025-04-20T17:21:44.172446Z"
    },
    "papermill": {
     "duration": 0.147007,
     "end_time": "2025-04-20T17:21:44.175092",
     "exception": false,
     "start_time": "2025-04-20T17:21:44.028085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../input/btlaionkk/data_onkk_merged.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753f6afe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T17:21:44.186734Z",
     "iopub.status.busy": "2025-04-20T17:21:44.186403Z",
     "iopub.status.idle": "2025-04-20T17:21:44.209443Z",
     "shell.execute_reply": "2025-04-20T17:21:44.208616Z"
    },
    "papermill": {
     "duration": 0.030633,
     "end_time": "2025-04-20T17:21:44.211030",
     "exception": false,
     "start_time": "2025-04-20T17:21:44.180397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess_data(df):\n",
    "    df = df.copy()\n",
    "    full_dates = pd.date_range(start=df['time'].min(), end=df['time'].max(), freq='D')\n",
    "    station_data_list = []\n",
    "    for station_id in tqdm(df.ID.unique()):\n",
    "        station_data = df[df.ID == station_id].copy()\n",
    "        if len(station_data) == 0: continue\n",
    "        station_data['time']=pd.to_datetime(station_data['time'])\n",
    "        station_data_daily = station_data.set_index('time').reindex(full_dates).rename_axis('time').reset_index()\n",
    "\n",
    "        ### Preprocess time-dependent features\n",
    "        station_data_daily['pm25_lag1'] = station_data_daily.pm25 - station_data_daily.pm25.shift(1)\n",
    "        \n",
    "        station_data_daily['lat'] = np.nanmean(station_data_daily['lat'].values)\n",
    "        station_data_daily['lon'] = np.nanmean(station_data_daily['lon'].values)\n",
    "        station_data_daily['ID'] = np.nanmean(station_data_daily['ID'].values)\n",
    "\n",
    "        ### Gather station data\n",
    "        station_data_list += [station_data_daily]\n",
    "\n",
    "    df = pd.concat(station_data_list, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    ### Preprocess time-independent features\n",
    "    df['WDIR_x'] = np.cos(np.radians(df['WDIR']))\n",
    "    df['WDIR_y'] = np.sin(np.radians(df['WDIR']))\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df[\"day_of_year\"] = df[\"time\"].dt.dayofyear\n",
    "    df[\"sin_day\"] = np.sin(2 * np.pi * df[\"day_of_year\"] / 365)\n",
    "    df[\"cos_day\"] = np.cos(2 * np.pi * df[\"day_of_year\"] / 365)\n",
    "    df['wind_u'] = df['WSPD'] * np.cos(np.radians(df['WDIR']))\n",
    "    df['wind_v'] = df['WSPD'] * np.sin(np.radians(df['WDIR']))\n",
    "    df['temp_range'] = df['TX'] - df['TN']\n",
    "    \n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df['day_of_week'] = df['time'].dt.dayofweek\n",
    "    df['month'] = df['time'].dt.month\n",
    "    \n",
    "    def get_season(month):\n",
    "        if month in [12, 1, 2]:\n",
    "            return '4'\n",
    "        elif month in [3, 4, 5]:\n",
    "            return '1'\n",
    "        elif month in [6, 7, 8]:\n",
    "            return '2'\n",
    "        elif month in [9, 10, 11]:\n",
    "            return '3'\n",
    "            \n",
    "    df['season'] = df['month'].apply(get_season).astype(int)\n",
    "    df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    df['heat_index'] = df['TMP'] * df['RH']\n",
    "    \n",
    "    def calculate_dew_point(temp, rh):\n",
    "        a = 17.27\n",
    "        b = 237.7\n",
    "        gamma = np.log(rh / 100.0) + (a * temp) / (b + temp)\n",
    "        dew_point = (b * gamma) / (a - gamma)\n",
    "        return dew_point\n",
    "        \n",
    "    df['dew_point'] = df.apply(lambda row: calculate_dew_point(row['TMP'], row['RH']), axis=1)\n",
    "    \n",
    "    hanoi_lat, hanoi_lon = 21.0278, 105.8342\n",
    "    def haversine_distance(row, lat2, lon2):\n",
    "        lat1, lon1 = row['lat'], row['lon']\n",
    "        lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "        \n",
    "        dlat = lat2 - lat1 \n",
    "        dlon = lon2 - lon1 \n",
    "        a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "        c = 2 * math.asin(math.sqrt(a))\n",
    "        r = 6371 \n",
    "        return c * r\n",
    "    \n",
    "    df['distance_to_hanoi'] = df.apply(lambda row: haversine_distance(row, hanoi_lat, hanoi_lon), axis=1)\n",
    "    \n",
    "    # df['inversion_strength'] = df['TX'] - df['TN']\n",
    "    df['temp_wind'] = df['TMP'] * df['WSPD']\n",
    "    df['rh_pressure'] = df['RH'] * df['PRES2M']\n",
    "    df['wspd_squared']= df['WSPD'] ** 2\n",
    "\n",
    "    full_dates = pd.date_range(start=df['time'].min(), end=df['time'].max(), freq='D')\n",
    "    \n",
    "    df = df.copy()\n",
    "    station_data_list = []\n",
    "    for station_id in tqdm(df.ID.unique()):\n",
    "        station_data = df[df.ID == station_id].copy()\n",
    "        if len(station_data) == 0: continue\n",
    "        # full_dates = pd.date_range(start=station_data['time'].min(), end=station_data['time'].max(), freq='D')\n",
    "        station_data['time']=pd.to_datetime(station_data['time'])\n",
    "        # station_data_daily = station_data.set_index('time').reindex(full_dates).rename_axis('time').reset_index()\n",
    "        station_data_daily = station_data.set_index('time').rename_axis('time').reset_index()\n",
    "\n",
    "        ### Preprocess time-dependent features\n",
    "        for ft_name in station_data_daily.columns:\n",
    "            if ft_name not in [\n",
    "                'CO_column_number_density', 'Cloud', 'NO2_column_number_density',\n",
    "                'O3_column_number_density', 'absorbing_aerosol_index',\n",
    "            ]: continue\n",
    "            if station_data_daily[ft_name].dtype not in ['float', 'int']:  continue\n",
    "            if ft_name in ['pm25', 'lat', 'lon', 'time', 'ID']:  continue\n",
    "            station_data_daily[f'{ft_name}_prev1'] = station_data_daily[ft_name].shift(1)\n",
    "            station_data_daily[f'{ft_name}_next1'] = station_data_daily[ft_name].shift(-1)\n",
    "\n",
    "        ### Gather station data\n",
    "        station_data_list += [station_data_daily]\n",
    "\n",
    "    df = pd.concat(station_data_list, axis=0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_timeseries_data(df, window_size=16, show_tqdm=True):\n",
    "    ### Assume these 2 vars have been setup\n",
    "    global features, scaler\n",
    "    full_dates = pd.date_range(start=df['time'].min(), end=df['time'].max(), freq='D')\n",
    "\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    pbar = df.ID.unique()\n",
    "    if show_tqdm: pbar = tqdm(pbar)\n",
    "    for station_id in pbar:\n",
    "        station_data = df[df.ID == station_id].copy()\n",
    "        if len(station_data) == 0: continue\n",
    "        station_data['time']=pd.to_datetime(station_data['time'])\n",
    "        station_data_daily = station_data.set_index('time').reindex(full_dates).rename_axis('time').reset_index()\n",
    "    \n",
    "        for i in range(len(station_data_daily) - window_size):\n",
    "            # Do scaling stuff\n",
    "            currX = scaler.transform(station_data_daily[features].values[i:i+window_size])\n",
    "            curry = scaler.transform(station_data_daily[features].values[[i+window_size]])[0]\n",
    "\n",
    "            # Let the pm25 be in the first column\n",
    "            pm25_idx = features.index('pm25')\n",
    "            curry = curry[[pm25_idx] + [i for i in range(len(features)) if i != pm25_idx]]\n",
    "            \n",
    "            if np.isnan(np.sum(currX)) or np.isnan(np.sum(curry)):\n",
    "                continue\n",
    "            Xs += [currX]\n",
    "            ys += [curry]\n",
    "\n",
    "    X = np.stack(Xs)\n",
    "    y = np.stack(ys)\n",
    "    return X, y\n",
    "\n",
    "def create_timeseries_data_missing(df, window_size=8, show_tqdm=True):\n",
    "    ### Assume these 2 vars have been setup\n",
    "    global features, scaler\n",
    "    full_dates = pd.date_range(start=df['time'].min(), end=df['time'].max(), freq='D')\n",
    "\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    pbar = data.ID.unique()\n",
    "    if show_tqdm: pbar = tqdm(pbar)\n",
    "    for station_id in pbar:\n",
    "        station_data = df[df.ID == station_id].copy()\n",
    "        if len(station_data) == 0: continue\n",
    "        station_data['time']=pd.to_datetime(station_data['time'])\n",
    "        # station_data_daily = station_data.set_index('time').reindex(full_dates).rename_axis('time').reset_index()\n",
    "        station_data_daily = station_data.set_index('time').rename_axis('time').reset_index()\n",
    "\n",
    "        \n",
    "        currX = scaler.transform(station_data_daily[features].values)\n",
    "        # currX[:, 1] = np.nanmean(currX[:, 1])\n",
    "        # currX[:, 2] = np.nanmean(currX[:, 2])\n",
    "        Xs += [currX[:, None]]\n",
    "    Xs = np.concatenate(Xs, axis=1)\n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a076e882",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T17:21:44.222852Z",
     "iopub.status.busy": "2025-04-20T17:21:44.222522Z",
     "iopub.status.idle": "2025-04-20T17:21:45.355142Z",
     "shell.execute_reply": "2025-04-20T17:21:45.354111Z"
    },
    "papermill": {
     "duration": 1.140356,
     "end_time": "2025-04-20T17:21:45.356767",
     "exception": false,
     "start_time": "2025-04-20T17:21:44.216411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_processed = preprocess_data(data)\n",
    "data_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0457f35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T17:21:45.369700Z",
     "iopub.status.busy": "2025-04-20T17:21:45.369346Z",
     "iopub.status.idle": "2025-04-20T17:21:45.380687Z",
     "shell.execute_reply": "2025-04-20T17:21:45.379882Z"
    },
    "papermill": {
     "duration": 0.01978,
     "end_time": "2025-04-20T17:21:45.382389",
     "exception": false,
     "start_time": "2025-04-20T17:21:45.362609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_date = '2021-06-01'\n",
    "test_date = '2021-08-01'\n",
    "train = data_processed[data_processed['time'] < val_date]\n",
    "val = data_processed[(data_processed['time'] >= val_date) & (data_processed['time'] < test_date)]\n",
    "test = data_processed[data_processed['time'] >= test_date]\n",
    "#train = train.drop('ID',axis=1)\n",
    "#train = train.drop('time',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287cb994",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T17:21:45.395806Z",
     "iopub.status.busy": "2025-04-20T17:21:45.395465Z",
     "iopub.status.idle": "2025-04-20T17:21:45.401401Z",
     "shell.execute_reply": "2025-04-20T17:21:45.400586Z"
    },
    "papermill": {
     "duration": 0.013915,
     "end_time": "2025-04-20T17:21:45.402786",
     "exception": false,
     "start_time": "2025-04-20T17:21:45.388871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ed64f",
   "metadata": {
    "papermill": {
     "duration": 0.005709,
     "end_time": "2025-04-20T17:21:45.414391",
     "exception": false,
     "start_time": "2025-04-20T17:21:45.408682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce52a00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T17:21:45.427744Z",
     "iopub.status.busy": "2025-04-20T17:21:45.427408Z",
     "iopub.status.idle": "2025-04-20T17:21:45.444523Z",
     "shell.execute_reply": "2025-04-20T17:21:45.443373Z"
    },
    "papermill": {
     "duration": 0.025898,
     "end_time": "2025-04-20T17:21:45.446069",
     "exception": false,
     "start_time": "2025-04-20T17:21:45.420171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    # 'time', 'ID',\n",
    "    'pm25',\n",
    "    'lat', 'lon',\n",
    "    'sin_day', 'cos_day',\n",
    "    'SQRT_SEA_DEM_LAT', 'WSPD', 'WDIR',\n",
    "    'TMP', 'TX', 'TN', 'TP', 'RH', 'PRES2M',\n",
    "    # 'pm25_lag1',\n",
    "    # 'WDIR_x', 'WDIR_y',\n",
    "    # 'day_of_year',\n",
    "    'wind_u', 'wind_v',\n",
    "    # 'temp_range',\n",
    "    # 'day_of_week', 'month', 'season', 'is_weekend',\n",
    "    'heat_index', 'dew_point', 'distance_to_hanoi', 'temp_wind', 'rh_pressure',\n",
    "    # 'wspd_squared',\n",
    "    'CO_column_number_density', 'Cloud', 'NO2_column_number_density',\n",
    "    'O3_column_number_density', 'absorbing_aerosol_index',\n",
    "]\n",
    "\n",
    "\n",
    "### Fit a shared scaler on training data\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6349d230",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T17:21:45.459739Z",
     "iopub.status.busy": "2025-04-20T17:21:45.459386Z",
     "iopub.status.idle": "2025-04-20T17:21:45.464982Z",
     "shell.execute_reply": "2025-04-20T17:21:45.464004Z"
    },
    "papermill": {
     "duration": 0.014034,
     "end_time": "2025-04-20T17:21:45.466460",
     "exception": false,
     "start_time": "2025-04-20T17:21:45.452426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    \"pm25\",\n",
    "    \n",
    "    \"SQRT_SEA_DEM_LAT\",\n",
    "    \n",
    "    \"TN\", \"dew_point\", \"heat_index\", \"TMP\", \"sin_day\", \"PRES2M\",\n",
    "    \"distance_to_hanoi\", \"temp_wind\", \"cos_day\", \"TP\", \"TX\", \"wind_u\",\n",
    "    \"rh_pressure\",\n",
    "\n",
    "    'CO_column_number_density', 'Cloud', 'NO2_column_number_density',\n",
    "    'O3_column_number_density', 'absorbing_aerosol_index',\n",
    "]\n",
    "selected_features_indices = [i for i in range(len(features)) if features[i] in selected_features]\n",
    "print(selected_features_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5082c3",
   "metadata": {
    "papermill": {
     "duration": 0.006078,
     "end_time": "2025-04-20T17:21:45.478841",
     "exception": false,
     "start_time": "2025-04-20T17:21:45.472763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bfb32f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T17:21:45.492218Z",
     "iopub.status.busy": "2025-04-20T17:21:45.491872Z",
     "iopub.status.idle": "2025-04-20T17:21:45.501918Z",
     "shell.execute_reply": "2025-04-20T17:21:45.501123Z"
    },
    "papermill": {
     "duration": 0.018544,
     "end_time": "2025-04-20T17:21:45.503484",
     "exception": false,
     "start_time": "2025-04-20T17:21:45.484940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df=None, X_np=None, window_sizes=[16, 32, 64], batch_size=16):\n",
    "        \"\"\"\n",
    "        Creates a dataset that precomputes timeseries data for multiple window sizes.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): The input dataframe.\n",
    "            window_sizes (list): A list of integers specifying window sizes.\n",
    "        \"\"\"\n",
    "        self.window_sizes = window_sizes\n",
    "        self.batch_size = batch_size\n",
    "        self.data_dict = {}\n",
    "        if df is not None:\n",
    "            self.df = df\n",
    "            \n",
    "            # Precompute timeseries data for each window size and store in a dictionary.\n",
    "            for ws in tqdm(window_sizes):\n",
    "                X, y = create_timeseries_data(df, window_size=ws, show_tqdm=False)\n",
    "                self.data_dict[ws] = {'X': X, 'y': y}\n",
    "        else:\n",
    "            T, S, N = X_np.shape\n",
    "\n",
    "            for ws in tqdm(window_sizes):\n",
    "                Xs = []\n",
    "                ys = []\n",
    "                for s in range(S):\n",
    "                    for t in range(T - 1):\n",
    "                        if t + ws >= T: continue\n",
    "                        if np.isnan(X_np[t:t+ws, s]).sum() or np.isnan(X_np[t + ws, s, :]).sum():\n",
    "                            continue\n",
    "                        Xs += [X_np[t:t+ws, s]]\n",
    "                        ys += [X_np[t + ws, s, :]]\n",
    "                Xs = np.stack(Xs)[:, :, selected_features_indices]\n",
    "                ys = np.stack(ys)[:, selected_features_indices]\n",
    "                self.data_dict[ws] = {'X': Xs, 'y': ys}\n",
    "\n",
    "    def __len__(self):\n",
    "        # Define length as the sum of all samples computed for all window sizes.\n",
    "        total = 0\n",
    "        for ws in self.window_sizes:\n",
    "            total += len(self.data_dict[ws]['X'])\n",
    "        return total // self.batch_size\n",
    "\n",
    "    def __getitem__(self, dummy_index):\n",
    "        if dummy_index > self.__len__(): raise StopIteration\n",
    "        ws = random.choice(self.window_sizes)\n",
    "        data = self.data_dict[ws]\n",
    "        # Randomly sample an index from the chosen data.\n",
    "        sample_idx = random.sample(range(len(data['X'])), k=self.batch_size)\n",
    "        sample = {\n",
    "            'X': data['X'][sample_idx],\n",
    "            'y': data['y'][sample_idx],\n",
    "        }\n",
    "        return sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82de250",
   "metadata": {
    "papermill": {
     "duration": 0.006078,
     "end_time": "2025-04-20T17:21:45.515920",
     "exception": false,
     "start_time": "2025-04-20T17:21:45.509842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Implement LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44650955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T17:21:45.529569Z",
     "iopub.status.busy": "2025-04-20T17:21:45.529183Z",
     "iopub.status.idle": "2025-04-20T17:21:45.535773Z",
     "shell.execute_reply": "2025-04-20T17:21:45.534918Z"
    },
    "papermill": {
     "duration": 0.015037,
     "end_time": "2025-04-20T17:21:45.537141",
     "exception": false,
     "start_time": "2025-04-20T17:21:45.522104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.0, ft_weight=1.0):\n",
    "        super(MyRNN, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.ft_weight = ft_weight\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, window_size, input_dim]\n",
    "        # out shape: [batch, window_size, hidden_dim]\n",
    "        out, _ = self.lstm(x)\n",
    "        \n",
    "        last_output = out[:, -1, :]  # shape: [batch, hidden_dim]\n",
    "        output = self.fc(last_output)  # shape: [batch, output_dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def compute_loss(self, x, y):\n",
    "        y_pred = self.forward(x)\n",
    "        loss = F.mse_loss(y_pred[:, 0], y[:, 0]) + self.ft_weight * F.mse_loss(y_pred, y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a8b94c",
   "metadata": {},
   "source": [
    "# Training Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb365f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T17:21:45.551186Z",
     "iopub.status.busy": "2025-04-20T17:21:45.550881Z",
     "iopub.status.idle": "2025-04-20T17:21:45.564967Z",
     "shell.execute_reply": "2025-04-20T17:21:45.564110Z"
    },
    "papermill": {
     "duration": 0.022622,
     "end_time": "2025-04-20T17:21:45.566333",
     "exception": false,
     "start_time": "2025-04-20T17:21:45.543711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "from copy import deepcopy\n",
    "import time\n",
    "def eval_rnn(model, dataset, detailed=False):\n",
    "    tick = time.time()\n",
    "    fin_loss_dict = {}\n",
    "    for ws in dataset.data_dict.keys():\n",
    "        X_full = dataset.data_dict[ws]['X']\n",
    "        y_full = dataset.data_dict[ws]['y']\n",
    "        batch_size = dataset.batch_size\n",
    "        y_preds_all = []\n",
    "        y_trues_all = []\n",
    "        \n",
    "        for i in range(0, len(X_full), batch_size):\n",
    "            X_batch = torch.tensor(X_full[i:i+batch_size], device=args.device, dtype=torch.float32)\n",
    "            y_batch = torch.tensor(y_full[i:i+batch_size], device=args.device, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                y_pred = model.forward(X_batch).detach().cpu().numpy()\n",
    "                y_true = y_batch.detach().cpu().numpy()\n",
    "                y_pred_inv = (y_pred[:, 0] - scaler.min_[0]) / scaler.scale_[0]\n",
    "                y_true_inv = (y_true[:, 0] - scaler.min_[0]) / scaler.scale_[0]\n",
    "            y_preds_all.extend(y_pred_inv.tolist())\n",
    "            y_trues_all.extend(y_true_inv.tolist())\n",
    "        \n",
    "        final_mse = mean_squared_error(y_preds_all, y_trues_all)\n",
    "        final_mae = mean_absolute_error(y_preds_all, y_trues_all)\n",
    "        final_r2 = r2_score(y_trues_all, y_preds_all)\n",
    "        fin_loss_dict[ws] = {'mse': final_mse, 'mae': final_mae, 'r2': final_r2}\n",
    "    # print(time.time() - tick)\n",
    "    if detailed: return fin_loss_dict\n",
    "    else:\n",
    "        return -np.mean([fin_loss_dict[ws]['mse'] for ws in fin_loss_dict])\n",
    "\n",
    "def train_rnn(model, train_dataset, val_dataset, test_dataset,\n",
    "                num_epochs=10):\n",
    "    torch.cuda.empty_cache()\n",
    "    manual_seed(SEED)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    prev_eval_score = -9999.0\n",
    "    best_eval_score = -9999.0\n",
    "    iteration = 0\n",
    "    mean_loss = -1\n",
    "    best_ckpt_state_dict = deepcopy(model.state_dict())\n",
    "\n",
    "    model.train().to(args.device)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        pbar = tqdm(train_dataset, position=0, leave=True)\n",
    "        for sample in pbar:\n",
    "            X = sample['X']\n",
    "            y = sample['y']\n",
    "            X_tensor = torch.tensor(X, device=args.device, dtype=torch.float32)\n",
    "            y_tensor = torch.tensor(y, device=args.device, dtype=torch.float32)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = model.compute_loss(X_tensor, y_tensor)\n",
    "            loss.backward()\n",
    "\n",
    "            total_norm = 0\n",
    "            for p in model.parameters():\n",
    "                try:\n",
    "                    param_norm = p.grad.detach().data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "                except: pass\n",
    "            total_norm = total_norm ** 0.5\n",
    "            optimizer.step()\n",
    "            \n",
    "            if mean_loss is None or mean_loss < 0: mean_loss = loss.item()\n",
    "            else: mean_loss = 0.9 * mean_loss + 0.1 * loss.item()\n",
    "\n",
    "\n",
    "                \n",
    "            if (iteration + 1) % 20 == 0:\n",
    "                if best_eval_score > -9999:\n",
    "                    pbar.set_description_str(\n",
    "                    f\"Loss: {mean_loss:.4f} | Best Val Score: {best_eval_score:.4f} | Val Score: {prev_eval_score:.4f}\\t\")\n",
    "                else:\n",
    "                    pbar.set_description_str(\n",
    "                    f\"Loss: {mean_loss:.4f}\\t\")\n",
    "            if (iteration + 1) % (len(train_dataset) // 5) == 0:\n",
    "                # Evaluate on test data.\n",
    "                eval_score = eval_rnn(model, val_dataset)\n",
    "                prev_eval_score = eval_score\n",
    "                if eval_score > best_eval_score:\n",
    "                    best_eval_score = eval_score\n",
    "                    best_ckpt_state_dict = deepcopy(model.state_dict())\n",
    "            iteration += 1\n",
    "    model.load_state_dict(best_ckpt_state_dict)\n",
    "    return model, best_eval_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668a4cb",
   "metadata": {
    "papermill": {
     "duration": 0.005943,
     "end_time": "2025-04-20T17:21:45.578647",
     "exception": false,
     "start_time": "2025-04-20T17:21:45.572704",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca05e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T17:21:45.591982Z",
     "iopub.status.busy": "2025-04-20T17:21:45.591671Z",
     "iopub.status.idle": "2025-04-20T17:21:45.602808Z",
     "shell.execute_reply": "2025-04-20T17:21:45.602015Z"
    },
    "papermill": {
     "duration": 0.019589,
     "end_time": "2025-04-20T17:21:45.604267",
     "exception": false,
     "start_time": "2025-04-20T17:21:45.584678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import numpy as np\n",
    "\n",
    "args = Namespace(\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    # Tắt gpu đi chạy 15 phút :v\n",
    "    seed=1902,\n",
    "    window_sizes = [1,2,3,4,5,7,14,28],\n",
    "    # window_sizes = [1,2,3,4,5,7],\n",
    "    batch_size=32,\n",
    "    num_layers=2,\n",
    "    dropout=0.0,\n",
    "    hidden_dim=100,\n",
    "    num_epochs=20,\n",
    "    weight_decay=1e-6,\n",
    "    lr=1e-3,\n",
    "    ft_weight=0.0,\n",
    ")\n",
    "\n",
    "def manual_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # if you are using GPU\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffbdaf6",
   "metadata": {
    "papermill": {
     "duration": 0.005907,
     "end_time": "2025-04-20T17:21:45.616596",
     "exception": false,
     "start_time": "2025-04-20T17:21:45.610689",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Setup datasets and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dd87ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T17:21:45.630192Z",
     "iopub.status.busy": "2025-04-20T17:21:45.629886Z",
     "iopub.status.idle": "2025-04-20T17:21:45.685356Z",
     "shell.execute_reply": "2025-04-20T17:21:45.684236Z"
    },
    "papermill": {
     "duration": 0.064466,
     "end_time": "2025-04-20T17:21:45.687218",
     "exception": false,
     "start_time": "2025-04-20T17:21:45.622752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "imputed_X_train_np = np.load('./imputed_data/imputed_X_train_np_dilate.npy')\n",
    "imputed_X_val_np = np.load('./imputed_data/imputed_X_val_np.npy')\n",
    "imputed_X_test_np = np.load('./imputed_data/imputed_X_test_np.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79511e3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T17:21:45.701597Z",
     "iopub.status.busy": "2025-04-20T17:21:45.701247Z",
     "iopub.status.idle": "2025-04-20T17:21:47.350035Z",
     "shell.execute_reply": "2025-04-20T17:21:47.348927Z"
    },
    "papermill": {
     "duration": 1.657386,
     "end_time": "2025-04-20T17:21:47.351565",
     "exception": false,
     "start_time": "2025-04-20T17:21:45.694179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(X_np=imputed_X_train_np, window_sizes=args.window_sizes, batch_size=args.batch_size)\n",
    "val_dataset = TimeSeriesDataset(X_np=imputed_X_val_np, window_sizes=args.window_sizes, batch_size=args.batch_size)\n",
    "test_dataset  = TimeSeriesDataset(X_np=imputed_X_test_np, window_sizes=args.window_sizes, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b883afa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T17:21:47.367614Z",
     "iopub.status.busy": "2025-04-20T17:21:47.367238Z",
     "iopub.status.idle": "2025-04-20T17:21:47.371334Z",
     "shell.execute_reply": "2025-04-20T17:21:47.370433Z"
    },
    "papermill": {
     "duration": 0.013651,
     "end_time": "2025-04-20T17:21:47.372829",
     "exception": false,
     "start_time": "2025-04-20T17:21:47.359178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_features = train_dataset.data_dict[args.window_sizes[0]]['X'].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882dabe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T17:21:47.388922Z",
     "iopub.status.busy": "2025-04-20T17:21:47.388606Z",
     "iopub.status.idle": "2025-04-20T17:30:02.443130Z",
     "shell.execute_reply": "2025-04-20T17:30:02.441933Z"
    },
    "papermill": {
     "duration": 495.064389,
     "end_time": "2025-04-20T17:30:02.444755",
     "exception": false,
     "start_time": "2025-04-20T17:21:47.380366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rnn_model = MyRNN(num_features, args.hidden_dim, num_features,\n",
    "                  num_layers=args.num_layers, dropout=args.dropout, ft_weight=args.ft_weight)\n",
    "rnn_model, best_eval_score = train_rnn(rnn_model, train_dataset, val_dataset, test_dataset,\n",
    "                num_epochs=args.num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab950ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T17:30:04.055518Z",
     "iopub.status.busy": "2025-04-20T17:30:04.055133Z",
     "iopub.status.idle": "2025-04-20T17:30:04.476131Z",
     "shell.execute_reply": "2025-04-20T17:30:04.474974Z"
    },
    "papermill": {
     "duration": 0.852693,
     "end_time": "2025-04-20T17:30:04.477917",
     "exception": false,
     "start_time": "2025-04-20T17:30:03.625224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "val_results_json = json.dumps(eval_rnn(rnn_model, val_dataset, detailed=True), indent=2)\n",
    "pd.read_json(val_results_json).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bcb583",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T17:30:05.259787Z",
     "iopub.status.busy": "2025-04-20T17:30:05.259329Z",
     "iopub.status.idle": "2025-04-20T17:30:05.837107Z",
     "shell.execute_reply": "2025-04-20T17:30:05.835764Z"
    },
    "papermill": {
     "duration": 0.955904,
     "end_time": "2025-04-20T17:30:05.839042",
     "exception": false,
     "start_time": "2025-04-20T17:30:04.883138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "test_results_json = json.dumps(eval_rnn(rnn_model, test_dataset, detailed=True), indent=2)\n",
    "pd.read_json(test_results_json).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e66306",
   "metadata": {
    "papermill": {
     "duration": 0.369515,
     "end_time": "2025-04-20T17:30:06.637077",
     "exception": false,
     "start_time": "2025-04-20T17:30:06.267562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2a7090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11936449,
     "datasetId": 7148771,
     "sourceId": 11489915,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 11370960,
     "datasetId": 6841890,
     "sourceId": 10992105,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 537.343208,
   "end_time": "2025-04-20T17:30:09.809693",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-20T17:21:12.466485",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
