{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359f9163",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T07:33:17.700822Z",
     "iopub.status.busy": "2025-04-29T07:33:17.700137Z",
     "iopub.status.idle": "2025-04-29T07:33:31.882180Z",
     "shell.execute_reply": "2025-04-29T07:33:31.881403Z"
    },
    "papermill": {
     "duration": 14.191729,
     "end_time": "2025-04-29T07:33:31.883642",
     "exception": false,
     "start_time": "2025-04-29T07:33:17.691913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import sys\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def manual_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # if you are suing GPU\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0bdde8",
   "metadata": {
    "papermill": {
     "duration": 0.007132,
     "end_time": "2025-04-29T07:33:31.897623",
     "exception": false,
     "start_time": "2025-04-29T07:33:31.890491",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Data & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d686d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T07:33:31.911349Z",
     "iopub.status.busy": "2025-04-29T07:33:31.910572Z",
     "iopub.status.idle": "2025-04-29T07:33:32.068434Z",
     "shell.execute_reply": "2025-04-29T07:33:32.067625Z"
    },
    "papermill": {
     "duration": 0.166108,
     "end_time": "2025-04-29T07:33:32.069944",
     "exception": false,
     "start_time": "2025-04-29T07:33:31.903836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../input/btlaionkk/data_onkk_merged.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500806cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T07:33:32.083925Z",
     "iopub.status.busy": "2025-04-29T07:33:32.083722Z",
     "iopub.status.idle": "2025-04-29T07:33:32.104031Z",
     "shell.execute_reply": "2025-04-29T07:33:32.103545Z"
    },
    "papermill": {
     "duration": 0.028629,
     "end_time": "2025-04-29T07:33:32.105161",
     "exception": false,
     "start_time": "2025-04-29T07:33:32.076532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess_data(df):\n",
    "    df = df.copy()\n",
    "    full_dates = pd.date_range(start=df['time'].min(), end=df['time'].max(), freq='D')\n",
    "    station_data_list = []\n",
    "    for station_id in tqdm(df.ID.unique()):\n",
    "        station_data = df[df.ID == station_id].copy()\n",
    "        if len(station_data) == 0: continue\n",
    "        station_data['time']=pd.to_datetime(station_data['time'])\n",
    "        station_data_daily = station_data.set_index('time').reindex(full_dates).rename_axis('time').reset_index()\n",
    "\n",
    "        ### Preprocess time-dependent features\n",
    "        station_data_daily['pm25_lag1'] = station_data_daily.pm25 - station_data_daily.pm25.shift(1)\n",
    "        \n",
    "        station_data_daily['lat'] = np.nanmean(station_data_daily['lat'].values)\n",
    "        station_data_daily['lon'] = np.nanmean(station_data_daily['lon'].values)\n",
    "        station_data_daily['ID'] = np.nanmean(station_data_daily['ID'].values)\n",
    "\n",
    "        ### Gather station data\n",
    "        station_data_list += [station_data_daily]\n",
    "\n",
    "    df = pd.concat(station_data_list, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    ### Preprocess time-independent features\n",
    "    df['WDIR_x'] = np.cos(np.radians(df['WDIR']))\n",
    "    df['WDIR_y'] = np.sin(np.radians(df['WDIR']))\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df[\"day_of_year\"] = df[\"time\"].dt.dayofyear\n",
    "    df[\"sin_day\"] = np.sin(2 * np.pi * df[\"day_of_year\"] / 365)\n",
    "    df[\"cos_day\"] = np.cos(2 * np.pi * df[\"day_of_year\"] / 365)\n",
    "    df['wind_u'] = df['WSPD'] * np.cos(np.radians(df['WDIR']))\n",
    "    df['wind_v'] = df['WSPD'] * np.sin(np.radians(df['WDIR']))\n",
    "    df['temp_range'] = df['TX'] - df['TN']\n",
    "    \n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df['day_of_week'] = df['time'].dt.dayofweek\n",
    "    df['month'] = df['time'].dt.month\n",
    "    \n",
    "    def get_season(month):\n",
    "        if month in [12, 1, 2]:\n",
    "            return '4'\n",
    "        elif month in [3, 4, 5]:\n",
    "            return '1'\n",
    "        elif month in [6, 7, 8]:\n",
    "            return '2'\n",
    "        elif month in [9, 10, 11]:\n",
    "            return '3'\n",
    "            \n",
    "    df['season'] = df['month'].apply(get_season).astype(int)\n",
    "    df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    df['heat_index'] = df['TMP'] * df['RH']\n",
    "    \n",
    "    def calculate_dew_point(temp, rh):\n",
    "        a = 17.27\n",
    "        b = 237.7\n",
    "        gamma = np.log(rh / 100.0) + (a * temp) / (b + temp)\n",
    "        dew_point = (b * gamma) / (a - gamma)\n",
    "        return dew_point\n",
    "        \n",
    "    df['dew_point'] = df.apply(lambda row: calculate_dew_point(row['TMP'], row['RH']), axis=1)\n",
    "    \n",
    "    hanoi_lat, hanoi_lon = 21.0278, 105.8342\n",
    "    def haversine_distance(row, lat2, lon2):\n",
    "        lat1, lon1 = row['lat'], row['lon']\n",
    "        lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "        \n",
    "        dlat = lat2 - lat1 \n",
    "        dlon = lon2 - lon1 \n",
    "        a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "        c = 2 * math.asin(math.sqrt(a))\n",
    "        r = 6371 \n",
    "        return c * r\n",
    "    \n",
    "    df['distance_to_hanoi'] = df.apply(lambda row: haversine_distance(row, hanoi_lat, hanoi_lon), axis=1)\n",
    "    \n",
    "    # df['inversion_strength'] = df['TX'] - df['TN']\n",
    "    df['temp_wind'] = df['TMP'] * df['WSPD']\n",
    "    df['rh_pressure'] = df['RH'] * df['PRES2M']\n",
    "    df['wspd_squared']= df['WSPD'] ** 2\n",
    "\n",
    "    full_dates = pd.date_range(start=df['time'].min(), end=df['time'].max(), freq='D')\n",
    "    \n",
    "    df = df.copy()\n",
    "    station_data_list = []\n",
    "    for station_id in tqdm(df.ID.unique()):\n",
    "        station_data = df[df.ID == station_id].copy()\n",
    "        if len(station_data) == 0: continue\n",
    "        # full_dates = pd.date_range(start=station_data['time'].min(), end=station_data['time'].max(), freq='D')\n",
    "        station_data['time']=pd.to_datetime(station_data['time'])\n",
    "        # station_data_daily = station_data.set_index('time').reindex(full_dates).rename_axis('time').reset_index()\n",
    "        station_data_daily = station_data.set_index('time').rename_axis('time').reset_index()\n",
    "\n",
    "        ### Preprocess time-dependent features\n",
    "        for ft_name in station_data_daily.columns:\n",
    "            if ft_name not in [\n",
    "                'CO_column_number_density', 'Cloud', 'NO2_column_number_density',\n",
    "                'O3_column_number_density', 'absorbing_aerosol_index',\n",
    "            ]: continue\n",
    "            if station_data_daily[ft_name].dtype not in ['float', 'int']:  continue\n",
    "            if ft_name in ['pm25', 'lat', 'lon', 'time', 'ID']:  continue\n",
    "            station_data_daily[f'{ft_name}_prev1'] = station_data_daily[ft_name].shift(1)\n",
    "            station_data_daily[f'{ft_name}_next1'] = station_data_daily[ft_name].shift(-1)\n",
    "\n",
    "        ### Gather station data\n",
    "        station_data_list += [station_data_daily]\n",
    "\n",
    "    df = pd.concat(station_data_list, axis=0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_timeseries_data(df, window_size=16, show_tqdm=True):\n",
    "    ### Assume these 2 vars have been setup\n",
    "    global features, scaler\n",
    "\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    pbar = df.ID.unique()\n",
    "    if show_tqdm: pbar = tqdm(pbar)\n",
    "    for station_id in pbar:\n",
    "        station_data = df[df.ID == station_id].copy()\n",
    "        if len(station_data) == 0: continue\n",
    "        station_data['time']=pd.to_datetime(station_data['time'])\n",
    "        full_dates = pd.date_range(start=station_data['time'].min(), end=station_data['time'].max(), freq='D')\n",
    "        station_data_daily = station_data.set_index('time').reindex(full_dates).rename_axis('time').reset_index()\n",
    "    \n",
    "        for i in range(len(station_data_daily) - window_size):\n",
    "            # Do scaling stuff\n",
    "            currX = scaler.transform(station_data_daily[features].iloc[i:i+window_size].values)\n",
    "            curry = scaler.transform(station_data_daily[features].iloc[[i+window_size]].values)[0]\n",
    "\n",
    "            # Let the pm25 be in the first column\n",
    "            pm25_idx = features.index('pm25')\n",
    "            curry = curry[[pm25_idx] + [i for i in range(len(features)) if i != pm25_idx]]\n",
    "            \n",
    "            if np.isnan(np.sum(currX)) or np.isnan(np.sum(curry)):\n",
    "                continue\n",
    "            Xs += [currX]\n",
    "            ys += [curry]\n",
    "\n",
    "    X = np.stack(Xs)\n",
    "    y = np.stack(ys)\n",
    "    return X, y\n",
    "\n",
    "def create_timeseries_data_missing(df, window_size=8, show_tqdm=True):\n",
    "    ### Assume these 2 vars have been setup\n",
    "    global features, scaler\n",
    "    full_dates = pd.date_range(start=df['time'].min(), end=df['time'].max(), freq='D')\n",
    "\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    pbar = data.ID.unique()\n",
    "    if show_tqdm: pbar = tqdm(pbar)\n",
    "    for station_id in pbar:\n",
    "        station_data = df[df.ID == station_id].copy()\n",
    "        if len(station_data) == 0: continue\n",
    "        station_data['time']=pd.to_datetime(station_data['time'])\n",
    "        # station_data_daily = station_data.set_index('time').reindex(full_dates).rename_axis('time').reset_index()\n",
    "        station_data_daily = station_data.set_index('time').rename_axis('time').reset_index()\n",
    "\n",
    "        \n",
    "        currX = scaler.transform(station_data_daily[features].values)\n",
    "        # currX[:, 1] = np.nanmean(currX[:, 1])\n",
    "        # currX[:, 2] = np.nanmean(currX[:, 2])\n",
    "        Xs += [currX[:, None]]\n",
    "    Xs = np.concatenate(Xs, axis=1)\n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7c0af3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T07:33:32.118528Z",
     "iopub.status.busy": "2025-04-29T07:33:32.118314Z",
     "iopub.status.idle": "2025-04-29T07:33:32.914665Z",
     "shell.execute_reply": "2025-04-29T07:33:32.913909Z"
    },
    "papermill": {
     "duration": 0.804285,
     "end_time": "2025-04-29T07:33:32.915822",
     "exception": false,
     "start_time": "2025-04-29T07:33:32.111537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_processed = preprocess_data(data)\n",
    "data_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74615841",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T07:33:32.930855Z",
     "iopub.status.busy": "2025-04-29T07:33:32.930091Z",
     "iopub.status.idle": "2025-04-29T07:33:32.941428Z",
     "shell.execute_reply": "2025-04-29T07:33:32.940645Z"
    },
    "papermill": {
     "duration": 0.019813,
     "end_time": "2025-04-29T07:33:32.942659",
     "exception": false,
     "start_time": "2025-04-29T07:33:32.922846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_date = '2021-06-01'\n",
    "test_date = '2021-08-01'\n",
    "train = data_processed[data_processed['time'] < val_date]\n",
    "val = data_processed[(data_processed['time'] >= val_date) & (data_processed['time'] < test_date)]\n",
    "test = data_processed[data_processed['time'] >= test_date]\n",
    "#train = train.drop('ID',axis=1)\n",
    "#train = train.drop('time',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981142fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T07:33:32.956547Z",
     "iopub.status.busy": "2025-04-29T07:33:32.956301Z",
     "iopub.status.idle": "2025-04-29T07:33:32.962136Z",
     "shell.execute_reply": "2025-04-29T07:33:32.961426Z"
    },
    "papermill": {
     "duration": 0.014144,
     "end_time": "2025-04-29T07:33:32.963264",
     "exception": false,
     "start_time": "2025-04-29T07:33:32.949120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_processed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95095d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T07:33:32.977970Z",
     "iopub.status.busy": "2025-04-29T07:33:32.977422Z",
     "iopub.status.idle": "2025-04-29T07:33:32.991110Z",
     "shell.execute_reply": "2025-04-29T07:33:32.990553Z"
    },
    "papermill": {
     "duration": 0.021953,
     "end_time": "2025-04-29T07:33:32.992101",
     "exception": false,
     "start_time": "2025-04-29T07:33:32.970148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    ### Features selection\n",
    "    # 'time', 'ID',\n",
    "    'pm25',\n",
    "    'lat', 'lon',\n",
    "    'sin_day', 'cos_day',\n",
    "    'SQRT_SEA_DEM_LAT', 'WSPD',\n",
    "    'WDIR',\n",
    "    'TMP',\n",
    "    'TX', 'TN', 'TP', 'RH', 'PRES2M',\n",
    "    # 'pm25_lag1',\n",
    "    # 'WDIR_x', 'WDIR_y',\n",
    "    # 'day_of_year',\n",
    "    'wind_u', 'wind_v',\n",
    "    # 'temp_range',\n",
    "    # 'day_of_week', 'month', 'season', 'is_weekend',\n",
    "    'heat_index', 'dew_point', 'distance_to_hanoi', 'temp_wind',\n",
    "    'rh_pressure',\n",
    "    # 'wspd_squared',\n",
    "    'CO_column_number_density', 'Cloud', 'NO2_column_number_density',\n",
    "    'O3_column_number_density', 'absorbing_aerosol_index',\n",
    "]\n",
    "\n",
    "### Fit a shared scaler on training data\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cf6481",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T07:33:33.006655Z",
     "iopub.status.busy": "2025-04-29T07:33:33.006429Z",
     "iopub.status.idle": "2025-04-29T07:33:33.010963Z",
     "shell.execute_reply": "2025-04-29T07:33:33.010398Z"
    },
    "papermill": {
     "duration": 0.013059,
     "end_time": "2025-04-29T07:33:33.012025",
     "exception": false,
     "start_time": "2025-04-29T07:33:32.998966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    \"pm25\",\n",
    "    \n",
    "    \"SQRT_SEA_DEM_LAT\",\n",
    "    \n",
    "    \"TN\", \"dew_point\", \"heat_index\", \"TMP\", \"sin_day\", \"PRES2M\",\n",
    "    \"distance_to_hanoi\", \"temp_wind\", \"cos_day\", \"TP\", \"TX\", \"wind_u\",\n",
    "    \"rh_pressure\",\n",
    "\n",
    "    'CO_column_number_density', 'Cloud', 'NO2_column_number_density',\n",
    "    'O3_column_number_density', 'absorbing_aerosol_index',\n",
    "]\n",
    "\n",
    "print([ft for ft in features if ft not in selected_features])\n",
    "print([ft for ft in selected_features if ft not in features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ebccb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T07:33:33.026694Z",
     "iopub.status.busy": "2025-04-29T07:33:33.026466Z",
     "iopub.status.idle": "2025-04-29T07:33:33.030766Z",
     "shell.execute_reply": "2025-04-29T07:33:33.030238Z"
    },
    "papermill": {
     "duration": 0.012697,
     "end_time": "2025-04-29T07:33:33.031836",
     "exception": false,
     "start_time": "2025-04-29T07:33:33.019139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos_embed_indices = [1,2,3,4]\n",
    "np.array(features)[pos_embed_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d9198b",
   "metadata": {
    "papermill": {
     "duration": 0.006717,
     "end_time": "2025-04-29T07:33:33.045295",
     "exception": false,
     "start_time": "2025-04-29T07:33:33.038578",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5636b2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T07:33:33.059556Z",
     "iopub.status.busy": "2025-04-29T07:33:33.059340Z",
     "iopub.status.idle": "2025-04-29T07:33:33.142638Z",
     "shell.execute_reply": "2025-04-29T07:33:33.142138Z"
    },
    "papermill": {
     "duration": 0.09174,
     "end_time": "2025-04-29T07:33:33.143764",
     "exception": false,
     "start_time": "2025-04-29T07:33:33.052024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_np = create_timeseries_data_missing(train, show_tqdm=False)\n",
    "station_locs = X_train_np[0, :, [1, 2]]\n",
    "import numpy as np\n",
    "\n",
    "coords = station_locs.T   # shape â†’ (26, 2)\n",
    "\n",
    "diff    = coords[:, np.newaxis, :] - coords[np.newaxis, :, :]   # â†’ (26,26,2)\n",
    "sqdist  = np.sum(diff**2, axis=2)                               # â†’ (26,26)\n",
    "\n",
    "np.fill_diagonal(sqdist, np.inf)\n",
    "\n",
    "nearest_stations = np.argsort(sqdist, axis=1)[:, :9]  # shape â†’ (26,4)\n",
    "\n",
    "nearest_stations = nearest_stations.tolist()\n",
    "stations_sampling = [[i] + nearest_stations[i] for i in range(26)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd45630",
   "metadata": {
    "papermill": {
     "duration": 0.006725,
     "end_time": "2025-04-29T07:33:33.157813",
     "exception": false,
     "start_time": "2025-04-29T07:33:33.151088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2fc1a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T07:33:33.174239Z",
     "iopub.status.busy": "2025-04-29T07:33:33.174031Z",
     "iopub.status.idle": "2025-04-29T07:33:33.189458Z",
     "shell.execute_reply": "2025-04-29T07:33:33.188900Z"
    },
    "papermill": {
     "duration": 0.025682,
     "end_time": "2025-04-29T07:33:33.190555",
     "exception": false,
     "start_time": "2025-04-29T07:33:33.164873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df, window_sizes=[16, 32, 64], batch_size=16):\n",
    "        \"\"\"\n",
    "        Creates a dataset that precomputes timeseries data for multiple window sizes.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): The input dataframe.\n",
    "            window_sizes (list): A list of integers specifying window sizes.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.window_sizes = window_sizes\n",
    "        self.batch_size = batch_size\n",
    "        self.data_dict = {}\n",
    "        \n",
    "        # Precompute timeseries data for each window size and store in a dictionary.\n",
    "        for ws in tqdm(window_sizes):\n",
    "            X, y = create_timeseries_data(df, window_size=ws, show_tqdm=False)\n",
    "            self.data_dict[ws] = {'X': X, 'y': y}\n",
    "\n",
    "    def __len__(self):\n",
    "        # Define length as the sum of all samples computed for all window sizes.\n",
    "        total = 0\n",
    "        for ws in self.window_sizes:\n",
    "            total += len(self.data_dict[ws]['X'])\n",
    "        return total // self.batch_size\n",
    "\n",
    "    def __getitem__(self, dummy_index):\n",
    "        if dummy_index > self.__len__(): raise StopIteration\n",
    "        ws = random.choice(self.window_sizes)\n",
    "        data = self.data_dict[ws]\n",
    "        # Randomly sample an index from the chosen data.\n",
    "        sample_idx = random.sample(range(len(data['X'])), k=self.batch_size)\n",
    "        sample = {\n",
    "            'X': data['X'][sample_idx],\n",
    "            'y': data['y'][sample_idx],\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "\n",
    "class ImputeDataset(Dataset):\n",
    "    def __init__(self, df, time_window_size=7, station_window_size=5, sampling_size=10, additional_mask_probs=0.3):\n",
    "        \"\"\"\n",
    "        Creates a dataset that precomputes timeseries data for multiple window sizes.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): The input dataframe.\n",
    "            window_sizes (list): A list of integers specifying window sizes.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "\n",
    "        self.time_window_size=time_window_size\n",
    "        self.station_window_size=station_window_size\n",
    "        \n",
    "        self.X = create_timeseries_data_missing(df, show_tqdm=False)\n",
    "        self.mask = ~np.isnan(self.X)\n",
    "        self.additional_mask = (np.random.rand(*self.mask.shape) > additional_mask_probs) & self.mask\n",
    "        self.X[~self.mask] = 0.0\n",
    "\n",
    "        self.sample_indices = []\n",
    "\n",
    "        self.sampling_size = sampling_size\n",
    "\n",
    "        self._prepare()\n",
    "\n",
    "    def _prepare(self, ):\n",
    "        T, S, N = self.X.shape\n",
    "        for i in range(T):\n",
    "            if i + self.time_window_size >= T: break\n",
    "            time_indices = np.arange(i, i + self.time_window_size)\n",
    "            for _ in range(self.sampling_size):\n",
    "                station_indices = random.sample(\n",
    "                    random.choice(stations_sampling), k=self.station_window_size)\n",
    "                mask_batch = self.mask[time_indices][:, station_indices]\n",
    "                if mask_batch.mean() < 0.5: continue\n",
    "                self.sample_indices.append([time_indices, station_indices])\n",
    "                \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_indices)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        time_indices, station_indices = self.sample_indices[i]\n",
    "        batch = torch.tensor(\n",
    "            self.X[time_indices][:, station_indices], dtype=torch.float32)\n",
    "        mask_batch = torch.tensor(\n",
    "            self.mask[time_indices][:, station_indices], dtype=torch.float32)\n",
    "        additional_mask_batch = torch.tensor(\n",
    "            self.additional_mask[time_indices][:, station_indices], dtype=torch.float32)\n",
    "        \n",
    "        return (batch, mask_batch, additional_mask_batch)\n",
    "\n",
    "\n",
    "\n",
    "class ImputeBilevelDataset(Dataset):\n",
    "    def __init__(self, df, time_window_size=7, station_window_size=5, sampling_size=10, additional_mask_probs=0.0):\n",
    "        \"\"\"\n",
    "        Creates a dataset that precomputes timeseries data for multiple window sizes.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): The input dataframe.\n",
    "            window_sizes (list): A list of integers specifying window sizes.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "\n",
    "        self.time_window_size=time_window_size\n",
    "        self.station_window_size=station_window_size\n",
    "        \n",
    "        self.X = create_timeseries_data_missing(df, show_tqdm=False)\n",
    "        self.mask = ~np.isnan(self.X)\n",
    "        self.additional_mask = (np.random.rand(*self.mask.shape) > additional_mask_probs) & self.mask\n",
    "        self.X[~self.mask] = 0.0\n",
    "\n",
    "        self.sample_indices = []\n",
    "\n",
    "        self.sampling_size = sampling_size\n",
    "\n",
    "        self._prepare()\n",
    "\n",
    "    def _prepare(self, ):\n",
    "        T, S, N = self.X.shape\n",
    "        for i in range(T):\n",
    "            if i + self.time_window_size >= T: break\n",
    "            time_indices = np.arange(i, i + self.time_window_size)\n",
    "            for _ in range(self.sampling_size):\n",
    "                station_indices = random.sample(\n",
    "                    random.choice(stations_sampling), k=self.station_window_size)\n",
    "                mask_batch = self.mask[time_indices][:, station_indices]\n",
    "                if mask_batch[-1, :, 0].mean() < 1: continue\n",
    "                if mask_batch.mean() < 0.5: continue\n",
    "                self.sample_indices.append([time_indices, station_indices])\n",
    "                \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_indices)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        time_indices, station_indices = self.sample_indices[i]\n",
    "        batch = torch.tensor(\n",
    "            self.X[time_indices][:, station_indices], dtype=torch.float32)\n",
    "        mask_batch = torch.tensor(\n",
    "            self.mask[time_indices][:, station_indices], dtype=torch.float32)\n",
    "        additional_mask_batch = torch.tensor(\n",
    "            self.additional_mask[time_indices][:, station_indices], dtype=torch.float32)\n",
    "        \n",
    "        return (batch, mask_batch, additional_mask_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe50cfa9",
   "metadata": {
    "papermill": {
     "duration": 0.006944,
     "end_time": "2025-04-29T07:33:33.204407",
     "exception": false,
     "start_time": "2025-04-29T07:33:33.197463",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d764f63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T07:33:33.251876Z",
     "iopub.status.busy": "2025-04-29T07:33:33.251670Z",
     "iopub.status.idle": "2025-04-29T07:33:33.276183Z",
     "shell.execute_reply": "2025-04-29T07:33:33.275539Z"
    },
    "papermill": {
     "duration": 0.033313,
     "end_time": "2025-04-29T07:33:33.277202",
     "exception": false,
     "start_time": "2025-04-29T07:33:33.243889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b961de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T07:33:33.291963Z",
     "iopub.status.busy": "2025-04-29T07:33:33.291762Z",
     "iopub.status.idle": "2025-04-29T07:33:33.295745Z",
     "shell.execute_reply": "2025-04-29T07:33:33.295142Z"
    },
    "papermill": {
     "duration": 0.012794,
     "end_time": "2025-04-29T07:33:33.296823",
     "exception": false,
     "start_time": "2025-04-29T07:33:33.284029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mae import MAE, LSTMStudent\n",
    "from bilevel_impute import BiImpute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25415e04",
   "metadata": {},
   "source": [
    "# Training Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc090b40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T07:33:33.347868Z",
     "iopub.status.busy": "2025-04-29T07:33:33.347674Z",
     "iopub.status.idle": "2025-04-29T07:33:33.365019Z",
     "shell.execute_reply": "2025-04-29T07:33:33.364476Z"
    },
    "papermill": {
     "duration": 0.026134,
     "end_time": "2025-04-29T07:33:33.365994",
     "exception": false,
     "start_time": "2025-04-29T07:33:33.339860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "from copy import deepcopy\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def reset_all_parameters(module):\n",
    "    def reset_fn(m):\n",
    "        if hasattr(m, 'reset_parameters'):\n",
    "            m.reset_parameters()\n",
    "    module.apply(reset_fn)\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    model = deepcopy(model).eval().to(args.device)\n",
    "    tick = time.time()\n",
    "    losses = []\n",
    "    for sample in data_loader:\n",
    "        batch, mask_batch, additional_mask_batch = sample\n",
    "        batch = batch[0].to(args.device)\n",
    "        mask_batch = mask_batch[0].to(args.device)\n",
    "        additional_mask_batch = additional_mask_batch[0].to(args.device)\n",
    "\n",
    "        rec_mask = mask_batch - additional_mask_batch\n",
    "        \n",
    "        data_pred = model.forward_impute(batch, additional_mask_batch)\n",
    "        loss = ((data_pred - batch) ** 2 * rec_mask).sum() / (rec_mask.sum() + 1e-8)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "def train_bi(models, optimizers, train_loader, train_bilevel_loader, val_bilevel_loader, val_loader):\n",
    "    torch.cuda.empty_cache()\n",
    "    losses = []\n",
    "    prev_eval_score = -9999.0\n",
    "    best_eval_score = -9999.0\n",
    "    scores_list = []\n",
    "    args.global_step = 0\n",
    "    \n",
    "    teacher, student = models\n",
    "    teacher_optimizer, student_optimizer = optimizers\n",
    "    bi = BiImpute()\n",
    "    \n",
    "    train_iter = iter(train_loader)\n",
    "    def get_next_train_sample():\n",
    "        nonlocal train_iter\n",
    "        try: return next(train_iter)\n",
    "        except StopIteration:\n",
    "            train_iter = iter(train_loader)\n",
    "            return next(train_iter)\n",
    "    \n",
    "    train_bilevel_iter = iter(train_bilevel_loader)\n",
    "    def get_next_train_bi_sample():\n",
    "        nonlocal train_bilevel_iter\n",
    "        try: return next(train_bilevel_iter)\n",
    "        except StopIteration:\n",
    "            train_bilevel_iter = iter(train_bilevel_loader)\n",
    "            return next(train_bilevel_iter)\n",
    "    \n",
    "    val_bilevel_iter = iter(val_bilevel_loader)\n",
    "    def get_next_val_bi_sample():\n",
    "        nonlocal val_bilevel_iter\n",
    "        try: return next(val_bilevel_iter)\n",
    "        except StopIteration:\n",
    "            val_bilevel_iter = iter(val_bilevel_loader)\n",
    "            return next(val_bilevel_iter)\n",
    "        \n",
    "    pbar = tqdm(range(args.num_train_steps), position=0, leave=True)\n",
    "    for iteration in pbar:\n",
    "        teacher.train().to(args.device)\n",
    "        student.train().to(args.device)\n",
    "\n",
    "        t_batch, t_mask, _ = get_next_train_sample()\n",
    "        t_batch_bi, t_mask_bi, _ = get_next_train_bi_sample()\n",
    "        v_batch, v_mask, _ = get_next_val_bi_sample()\n",
    "        t_batch = t_batch[0].to(args.device)\n",
    "        t_mask = t_mask[0].to(args.device)\n",
    "        t_batch_bi = t_batch_bi[0].to(args.device)\n",
    "        t_mask_bi = t_mask_bi[0].to(args.device)\n",
    "        v_batch = v_batch[0].to(args.device)\n",
    "        v_mask = v_mask[0].to(args.device)\n",
    "\n",
    "        all_samples = [(t_batch, t_mask), (t_batch_bi, t_mask_bi), (v_batch, v_mask)]\n",
    "        bi.step_fn(args, models, optimizers, all_samples)\n",
    "\n",
    "        \n",
    "        if (iteration + 1) % 20 == 0:\n",
    "            if best_eval_score > -9999:\n",
    "                pbar.set_description_str(\n",
    "                f\"Best Val Score: {best_eval_score:.4f} | Val Score: {prev_eval_score:.4f}\\t\")\n",
    "        \n",
    "        if (iteration + 1) % 100 == 0:\n",
    "            \n",
    "            eval_score = -eval_model(teacher, val_loader)\n",
    "            prev_eval_score = eval_score\n",
    "            if eval_score > best_eval_score:\n",
    "                best_eval_score = eval_score\n",
    "                best_ckpt_state_dict = deepcopy(teacher.state_dict())\n",
    "                torch.save(teacher.state_dict(), 'best_model.pt')\n",
    "                # print(\"On val:\", best_eval_score)\n",
    "                print(\"On test:\", eval_model(teacher, test_loader))\n",
    "            \n",
    "            if best_eval_score > -9999:\n",
    "                print(f\"\"\"\n",
    "    Step {args.global_step}: Best Val Score: {best_eval_score:.4f} | Val Score: {prev_eval_score:.4f}\\t\n",
    "    s: ({bi.step_info['nlll/student_on_t']:.4f}, {bi.step_info['nlll/student_on_v']:.4f}), t: {bi.step_info['mae']:.4f}\n",
    "    \"\"\".strip(), end=', ')\n",
    "            else:\n",
    "                print(f\"\"\"\n",
    "    Step {args.global_step}:\n",
    "    s: ({bi.step_info['nlll/student_on_t']:.4f}, {bi.step_info['nlll/student_on_v']:.4f}), t: {bi.step_info['mae']:.4f}\n",
    "    \"\"\".strip(), end=', ')\n",
    "\n",
    "\n",
    "        if (iteration + 1) % 50000 == 0:\n",
    "            reset_all_parameters(student)\n",
    "            student_optimizer.state.clear()\n",
    "            student_optimizer.zero_grad()\n",
    "            bi.moving_dot_product = None\n",
    "            \n",
    "        \n",
    "\n",
    "        args.global_step += 1\n",
    "        \n",
    "    return teacher, best_eval_score\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader):\n",
    "    torch.cuda.empty_cache()\n",
    "    manual_seed(SEED)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    prev_eval_score = -9999.0\n",
    "    best_eval_score = -9999.0\n",
    "    iteration = 0\n",
    "    mean_loss = -1\n",
    "    best_ckpt_state_dict = deepcopy(model.state_dict())\n",
    "\n",
    "    model.train().to(args.device)\n",
    "    args.global_step = 0\n",
    "    for epoch in range(args.num_epochs):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        pbar = tqdm(train_loader, position=0, leave=True)\n",
    "        if args.global_step > args.num_pretrain_steps: break\n",
    "        for sample in pbar:\n",
    "            if args.global_step > args.num_pretrain_steps: break\n",
    "            batch, mask_batch, additional_mask_batch = sample\n",
    "            batch = batch[0].to(args.device)\n",
    "            mask_batch = mask_batch[0].to(args.device)\n",
    "            additional_mask_batch = additional_mask_batch[0].to(args.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.forward_ssl(batch, mask_batch, mask_ratio=args.mask_ratio)\n",
    "            loss.backward()\n",
    "\n",
    "            total_norm = 0\n",
    "            for p in model.parameters():\n",
    "                try:\n",
    "                    param_norm = p.grad.detach().data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "                except: pass\n",
    "            total_norm = total_norm ** 0.5\n",
    "            optimizer.step()\n",
    "            \n",
    "            if mean_loss is None or mean_loss < 0: mean_loss = loss.item()\n",
    "            else: mean_loss = 0.9 * mean_loss + 0.1 * loss.item()\n",
    "\n",
    "\n",
    "                \n",
    "            if (iteration + 1) % 20 == 0:\n",
    "                if best_eval_score > -9999:\n",
    "                    pbar.set_description_str(\n",
    "                    f\"Loss: {mean_loss:.4f} | Best Val Score: {best_eval_score:.4f} | Val Score: {prev_eval_score:.4f}\\t\")\n",
    "                else:\n",
    "                    pbar.set_description_str(\n",
    "                    f\"Loss: {mean_loss:.4f}\\t\")\n",
    "            if (iteration + 1) % 500 == 0:\n",
    "                # Evaluate on test data.\n",
    "                eval_score = -eval_model(model, val_loader)\n",
    "                prev_eval_score = eval_score\n",
    "                if eval_score > best_eval_score:\n",
    "                    best_eval_score = eval_score\n",
    "                    best_ckpt_state_dict = deepcopy(model.state_dict())\n",
    "                    torch.save(model.state_dict(), 'best_model_pretrain.pt')\n",
    "            iteration += 1\n",
    "            args.global_step += 1\n",
    "    model.load_state_dict(best_ckpt_state_dict)\n",
    "    return model, best_eval_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8291151",
   "metadata": {
    "papermill": {
     "duration": 0.006682,
     "end_time": "2025-04-29T07:33:33.379428",
     "exception": false,
     "start_time": "2025-04-29T07:33:33.372746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d758e3f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T07:33:33.438113Z",
     "iopub.status.busy": "2025-04-29T07:33:33.437222Z",
     "iopub.status.idle": "2025-04-29T07:33:33.503224Z",
     "shell.execute_reply": "2025-04-29T07:33:33.502712Z"
    },
    "papermill": {
     "duration": 0.118198,
     "end_time": "2025-04-29T07:33:33.504308",
     "exception": false,
     "start_time": "2025-04-29T07:33:33.386110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import numpy as np\n",
    "\n",
    "# args = Namespace(\n",
    "#     device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "#     seed=1902,\n",
    "#     ft_embed_dim=8,\n",
    "#     ft_enc_nhead=1,\n",
    "#     ft_enc_num_layers=3,\n",
    "#     mae_hidden_dim=64,\n",
    "#     mlp_ratio=4.,\n",
    "#     dim_feedforward=64 * 4,\n",
    "#     mae_nhead=4,\n",
    "#     mae_num_layers=3,\n",
    "#     mae_dropout=0.1,\n",
    "#     activation='gelu',\n",
    "#     time_window_size=5,\n",
    "#     station_window_size=5,\n",
    "#     num_epochs=2,\n",
    "#     lr=3e-4,\n",
    "#     weight_decay=1e-6,\n",
    "#     mask_ratio=0.3,\n",
    "# )\n",
    "\n",
    "args = Namespace( # Optuna\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    seed=1902,\n",
    "    ft_embed_dim=24,\n",
    "    ft_enc_nhead=2,\n",
    "    ft_enc_num_layers=4,\n",
    "    mae_hidden_dim=160,\n",
    "    mlp_ratio=4.36,\n",
    "    dim_feedforward=697,\n",
    "    mae_nhead=4,\n",
    "    mae_num_layers=3,\n",
    "    mae_dropout=0.29,\n",
    "    activation='gelu',\n",
    "    time_window_size=5,\n",
    "    station_window_size=5,\n",
    "    num_epochs=1,\n",
    "    lr=6.9e-5,\n",
    "    weight_decay=4.12e-6,\n",
    "    mask_ratio=0.31,\n",
    "\n",
    "    student_lr=0.006619339793756876,\n",
    "    student_weight_decay=3.903728867916741e-08,\n",
    "    student_lambda=0.38463245884031055,\n",
    "    student_dropout=0.36358270939782433,\n",
    "\n",
    "    num_train_steps=2000,\n",
    "    num_pretrain_steps=20000,\n",
    ")\n",
    "\n",
    "args.pos_embed_indices = pos_embed_indices\n",
    "args.num_features = len(features)\n",
    "\n",
    "def manual_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # if you are using GPU\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41aba58",
   "metadata": {
    "papermill": {
     "duration": 0.006697,
     "end_time": "2025-04-29T07:33:33.518274",
     "exception": false,
     "start_time": "2025-04-29T07:33:33.511577",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Setup datasets and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930e622e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T07:33:33.532545Z",
     "iopub.status.busy": "2025-04-29T07:33:33.532311Z",
     "iopub.status.idle": "2025-04-29T07:33:42.073633Z",
     "shell.execute_reply": "2025-04-29T07:33:42.072956Z"
    },
    "papermill": {
     "duration": 8.55,
     "end_time": "2025-04-29T07:33:42.074988",
     "exception": false,
     "start_time": "2025-04-29T07:33:33.524988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = ImputeDataset(train, args.time_window_size, args.station_window_size,\n",
    "                              sampling_size=200, additional_mask_probs=0.0)\n",
    "train_bilevel_dataset = ImputeDataset(train, args.time_window_size, args.station_window_size,\n",
    "                              sampling_size=200, additional_mask_probs=0.0)\n",
    "# train_bilevel_dataset = ImputeBilevelDataset(train, args.time_window_size, args.station_window_size,\n",
    "#                               sampling_size=200, additional_mask_probs=0.0)\n",
    "val_bilevel_dataset = ImputeBilevelDataset(val, args.time_window_size, args.station_window_size,\n",
    "                            sampling_size=200, additional_mask_probs=0.0)\n",
    "val_dataset = ImputeDataset(val, args.time_window_size, args.station_window_size,\n",
    "                            sampling_size=20, additional_mask_probs=0.3)\n",
    "test_dataset = ImputeDataset(test, args.time_window_size, args.station_window_size,\n",
    "                            sampling_size=10, additional_mask_probs=0.3)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "train_bilevel_loader = DataLoader(train_bilevel_dataset, batch_size=1, shuffle=True)\n",
    "val_bilevel_loader = DataLoader(val_bilevel_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "teacher = MAE(args)\n",
    "student = LSTMStudent(args)\n",
    "\n",
    "teacher_optimizer = optim.AdamW(teacher.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "student_optimizer = optim.AdamW(student.parameters(), lr=args.student_lr, weight_decay=args.student_weight_decay)\n",
    "\n",
    "models = (teacher, student)\n",
    "optimizers = (teacher_optimizer, student_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0d27d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T07:33:42.090457Z",
     "iopub.status.busy": "2025-04-29T07:33:42.090057Z",
     "iopub.status.idle": "2025-04-29T07:46:20.274387Z",
     "shell.execute_reply": "2025-04-29T07:46:20.273557Z"
    },
    "papermill": {
     "duration": 758.193345,
     "end_time": "2025-04-29T07:46:20.275797",
     "exception": false,
     "start_time": "2025-04-29T07:33:42.082452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "teacher, best_eval_score = train_model(teacher, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9066746",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T07:46:20.760870Z",
     "iopub.status.busy": "2025-04-29T07:46:20.760581Z",
     "iopub.status.idle": "2025-04-29T08:10:22.455244Z",
     "shell.execute_reply": "2025-04-29T08:10:22.454229Z"
    },
    "papermill": {
     "duration": 1441.939436,
     "end_time": "2025-04-29T08:10:22.456418",
     "exception": false,
     "start_time": "2025-04-29T07:46:20.516982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "teacher, best_eval_score = train_bi(models, optimizers, train_loader, train_bilevel_loader, val_bilevel_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e774ff79",
   "metadata": {},
   "source": [
    "# Load model and create imputed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62dd4e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T08:10:23.787196Z",
     "iopub.status.busy": "2025-04-29T08:10:23.786947Z",
     "iopub.status.idle": "2025-04-29T08:10:23.844730Z",
     "shell.execute_reply": "2025-04-29T08:10:23.843923Z"
    },
    "papermill": {
     "duration": 0.3808,
     "end_time": "2025-04-29T08:10:23.845866",
     "exception": false,
     "start_time": "2025-04-29T08:10:23.465066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model = deepcopy(teacher).eval().to(args.device)\n",
    "try:best_model.load_state_dict(torch.load('./best_model.pt'))\n",
    "except:\n",
    "    try: best_model.load_state_dict(torch.load('./best_model_pretrain.pt'))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da245923",
   "metadata": {},
   "source": [
    "### Calculate nearest stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c89e08f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T08:10:24.535024Z",
     "iopub.status.busy": "2025-04-29T08:10:24.534756Z",
     "iopub.status.idle": "2025-04-29T08:10:24.620257Z",
     "shell.execute_reply": "2025-04-29T08:10:24.619534Z"
    },
    "papermill": {
     "duration": 0.461799,
     "end_time": "2025-04-29T08:10:24.621598",
     "exception": false,
     "start_time": "2025-04-29T08:10:24.159799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_np = create_timeseries_data_missing(train, show_tqdm=False)\n",
    "station_locs = X_train_np[0, :, [1, 2]]\n",
    "import numpy as np\n",
    "\n",
    "coords = station_locs.T   # shape â†’ (26, 2)\n",
    "\n",
    "diff    = coords[:, np.newaxis, :] - coords[np.newaxis, :, :]   # â†’ (26,26,2)\n",
    "sqdist  = np.sum(diff**2, axis=2)                               # â†’ (26,26)\n",
    "\n",
    "np.fill_diagonal(sqdist, np.inf)\n",
    "\n",
    "nearest_stations = np.argsort(sqdist, axis=1)[:, :args.station_window_size - 1]  # shape â†’ (26,4)\n",
    "\n",
    "nearest_stations = nearest_stations.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c42b621",
   "metadata": {},
   "source": [
    "### ðŸ“Œ Imputation Rules\n",
    "\n",
    "To ensure the best use of incomplete data while maintaining strict evaluation integrity, we apply different imputation strategies for training, validation, and test sets. These strategies aim to **(1) reduce reconstruction error during training** and **(2) prevent information leakage during evaluation**.\n",
    "\n",
    "#### ðŸ”§ Training Set\n",
    "For the training set, our goal is to provide the downstream LSTM model with the most accurate reconstructed values. Therefore, **future data is allowed** during imputation to maximize the information available. Each missing value is filled using predictions from a masked autoencoder (MAE) applied on a spatiotemporal window that includes the target station and its nearest neighbors. The MAE is trained and selected based on validation MSE.\n",
    "\n",
    "We create multiple versions of the imputed training data:\n",
    "- `imputed_X_train_np`: Fully imputed version for inspection or optional use.\n",
    "- `imputed_X_train_np_dilate`: Only retains values that are supported by sufficiently reliable neighboring information.\n",
    "- `imputed_X_train_np_restrict`: Masks out values that were originally missing in the PM2.5 feature.\n",
    "\n",
    "The `imputed_X_train_np_dilate` version selectively retains imputed values based on local spatiotemporal support. For each station, we apply 1D `binary_dilation` over time to extend valid PM2.5 observations, then merge this mask with that of the stationâ€™s nearest neighbor. This ensures a value is only kept if the original data had support either at that station or nearby in time/space. All other imputed values are discarded by resetting them to `NaN`.\n",
    "\n",
    "#### ðŸ§ª Validation & Test Sets\n",
    "In contrast to training, **future data is strictly prohibited** during imputation on validation and test sets to prevent any leakage. Each sample is imputed **only using past and current data**, ensuring that the evaluation remains unbiased.\n",
    "\n",
    "For each time point `t`, only the time window `[t - W + 1, t]` is used (where `W` is the window size). The MAE model imputes missing values in this window using the most informative set of stations â€” consisting of the target station and its closest neighbors â€” as determined dynamically by mask coverage.\n",
    "\n",
    "Only the **last time step in each window is retained** after imputation, corresponding to the target input of the LSTM predictor. Furthermore, any validation/test sample where the target PM2.5 label is missing is **excluded from downstream evaluation** to maintain fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8265f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T08:10:25.259700Z",
     "iopub.status.busy": "2025-04-29T08:10:25.259135Z",
     "iopub.status.idle": "2025-04-29T08:11:26.588137Z",
     "shell.execute_reply": "2025-04-29T08:11:26.587317Z"
    },
    "papermill": {
     "duration": 61.651132,
     "end_time": "2025-04-29T08:11:26.589250",
     "exception": false,
     "start_time": "2025-04-29T08:10:24.938118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "           \n",
    "X_train_np = create_timeseries_data_missing(train, show_tqdm=False)\n",
    "mask_np = ~np.isnan(X_train_np)\n",
    "\n",
    "imputed_X_train_np = np.zeros_like(X_train_np) + np.nan\n",
    "mask_blocks = mask_np.mean(axis=-1) < 0.5\n",
    "mask_blocks.shape\n",
    "\n",
    "T, S, N = X_train_np.shape\n",
    "for t in tqdm(range(T)):\n",
    "    if t + args.time_window_size >= T: break\n",
    "    time_indices = np.arange(t, t + args.time_window_size)\n",
    "    for s in range(S):\n",
    "        if mask_blocks[t].mean() < 0.1: continue\n",
    "        best_station_indices = None\n",
    "        best_score = 0.0\n",
    "        station_indices = [s] + nearest_stations[s]\n",
    "        mask_batch = mask_np[time_indices][:, station_indices]\n",
    "        if mask_batch.mean() > best_score:\n",
    "            best_score = mask_batch.mean()\n",
    "            best_station_indices = deepcopy(station_indices)\n",
    "        if best_score < 0.5: \n",
    "            continue\n",
    "        station_indices = best_station_indices\n",
    "        batch = torch.tensor(\n",
    "            X_train_np[time_indices][:, station_indices], dtype=torch.float32, device=args.device)\n",
    "        batch = batch.nan_to_num(0.0)\n",
    "        mask_batch = torch.tensor(\n",
    "            mask_np[time_indices][:, station_indices], dtype=torch.float32, device=args.device)\n",
    "        with torch.no_grad():\n",
    "            data_pred = best_model.forward_impute(batch, mask_batch)\n",
    "            data_pred = data_pred * (1 - mask_batch) + batch * mask_batch\n",
    "        data_pred = data_pred.detach().cpu().numpy()\n",
    "        imputed_X_train_np[time_indices, s] = data_pred[:, 0]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984bee7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T08:11:27.321735Z",
     "iopub.status.busy": "2025-04-29T08:11:27.321029Z",
     "iopub.status.idle": "2025-04-29T08:11:34.019063Z",
     "shell.execute_reply": "2025-04-29T08:11:34.018286Z"
    },
    "papermill": {
     "duration": 7.09462,
     "end_time": "2025-04-29T08:11:34.020524",
     "exception": false,
     "start_time": "2025-04-29T08:11:26.925904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "           \n",
    "X_val_np = create_timeseries_data_missing(val, show_tqdm=False)\n",
    "mask_np = ~np.isnan(X_val_np)\n",
    "\n",
    "imputed_X_val_np = np.zeros_like(X_val_np) + np.nan\n",
    "mask_blocks = mask_np.mean(axis=-1) < 0.5\n",
    "mask_blocks.shape\n",
    "\n",
    "T, S, N = X_val_np.shape\n",
    "for t_reverse in tqdm(range(args.time_window_size - 1, T)):\n",
    "    t = t_reverse - args.time_window_size + 1\n",
    "    if t < 0: break\n",
    "    time_indices = np.arange(t, t + args.time_window_size)\n",
    "    for s in range(S):\n",
    "        if mask_blocks[t].mean() < 0.1: continue\n",
    "        best_station_indices = None\n",
    "        best_score = 0.0\n",
    "        station_indices = [s] + nearest_stations[s]\n",
    "        mask_batch = mask_np[time_indices][:, station_indices]\n",
    "        if mask_batch.mean() > best_score:\n",
    "            best_score = mask_batch.mean()\n",
    "            best_station_indices = deepcopy(station_indices)\n",
    "        if best_score < 0.5: \n",
    "            continue\n",
    "        station_indices = best_station_indices\n",
    "        batch = torch.tensor(\n",
    "            X_val_np[time_indices][:, station_indices], dtype=torch.float32, device=args.device)\n",
    "        batch = batch.nan_to_num(0.0)\n",
    "        mask_batch = torch.tensor(\n",
    "            mask_np[time_indices][:, station_indices], dtype=torch.float32, device=args.device)\n",
    "        with torch.no_grad():\n",
    "            data_pred = best_model.forward_impute(batch, mask_batch)\n",
    "            data_pred = data_pred * (1 - mask_batch) + batch * mask_batch\n",
    "        data_pred = data_pred.detach().cpu().numpy()\n",
    "        imputed_X_val_np[time_indices[-1], s] = data_pred[-1, 0]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c4d7e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T08:11:34.701032Z",
     "iopub.status.busy": "2025-04-29T08:11:34.700755Z",
     "iopub.status.idle": "2025-04-29T08:11:47.584332Z",
     "shell.execute_reply": "2025-04-29T08:11:47.583547Z"
    },
    "papermill": {
     "duration": 13.225438,
     "end_time": "2025-04-29T08:11:47.585414",
     "exception": false,
     "start_time": "2025-04-29T08:11:34.359976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "X_test_np = create_timeseries_data_missing(test, show_tqdm=False)\n",
    "mask_np = ~np.isnan(X_test_np)\n",
    "\n",
    "imputed_X_test_np = np.zeros_like(X_test_np) + np.nan\n",
    "mask_blocks = mask_np.mean(axis=-1) < 0.5\n",
    "mask_blocks.shape\n",
    "\n",
    "T, S, N = X_test_np.shape\n",
    "for t_reverse in tqdm(range(args.time_window_size - 1, T)):\n",
    "    t = t_reverse - args.time_window_size + 1\n",
    "    if t < 0: break\n",
    "    time_indices = np.arange(t, t + args.time_window_size)\n",
    "    for s in range(S):\n",
    "        if mask_blocks[t].mean() < 0.1: continue\n",
    "        best_station_indices = None\n",
    "        best_score = 0.0\n",
    "        station_indices = [s] + nearest_stations[s]\n",
    "        mask_batch = mask_np[time_indices][:, station_indices]\n",
    "        if mask_batch.mean() > best_score:\n",
    "            best_score = mask_batch.mean()\n",
    "            best_station_indices = deepcopy(station_indices)\n",
    "        if best_score < 0.5: \n",
    "            continue\n",
    "        station_indices = best_station_indices\n",
    "        batch = torch.tensor(\n",
    "            X_test_np[time_indices][:, station_indices], dtype=torch.float32, device=args.device)\n",
    "        batch = batch.nan_to_num(0.0)\n",
    "        mask_batch = torch.tensor(\n",
    "            mask_np[time_indices][:, station_indices], dtype=torch.float32, device=args.device)\n",
    "        with torch.no_grad():\n",
    "            data_pred = best_model.forward_impute(batch, mask_batch)\n",
    "            data_pred = data_pred * (1 - mask_batch) + batch * mask_batch\n",
    "        data_pred = data_pred.detach().cpu().numpy()\n",
    "        imputed_X_test_np[time_indices[-1], s] = data_pred[-1, 0]\n",
    "        \n",
    "    \n",
    "imputed_X_test_np_full = imputed_X_test_np.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b61d21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T08:11:48.346593Z",
     "iopub.status.busy": "2025-04-29T08:11:48.346286Z",
     "iopub.status.idle": "2025-04-29T08:11:48.357991Z",
     "shell.execute_reply": "2025-04-29T08:11:48.357380Z"
    },
    "papermill": {
     "duration": 0.426084,
     "end_time": "2025-04-29T08:11:48.359123",
     "exception": false,
     "start_time": "2025-04-29T08:11:47.933039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from scipy.ndimage import binary_dilation\n",
    "train_keep_mask = ~np.isnan(X_train_np[:, :, 0])[:, :]\n",
    "T, S = train_keep_mask.shape\n",
    "for s in range(S):\n",
    "    train_keep_mask[:, s] = binary_dilation(train_keep_mask[:, s])\n",
    "    \n",
    "    for s1 in nearest_stations[s][:1]:\n",
    "        train_keep_mask[:, s] |= train_keep_mask[:, s1]\n",
    "\n",
    "imputed_X_train_np_dilate = imputed_X_train_np.copy()\n",
    "imputed_X_train_np_restrict = imputed_X_train_np.copy()\n",
    "\n",
    "imputed_X_train_np_dilate[~train_keep_mask] = np.nan\n",
    "imputed_X_train_np_restrict[np.isnan(X_train_np[:, :, 0])[:, :]] = np.nan\n",
    "\n",
    "imputed_X_val_np[np.isnan(X_val_np[:, :, 0])[:, :]] = np.nan\n",
    "imputed_X_test_np[np.isnan(X_test_np[:, :, 0])[:, :]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5077fb66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T08:11:49.057242Z",
     "iopub.status.busy": "2025-04-29T08:11:49.056635Z",
     "iopub.status.idle": "2025-04-29T08:11:49.083731Z",
     "shell.execute_reply": "2025-04-29T08:11:49.083173Z"
    },
    "papermill": {
     "duration": 0.378473,
     "end_time": "2025-04-29T08:11:49.084888",
     "exception": false,
     "start_time": "2025-04-29T08:11:48.706415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('imputed_data', exist_ok=True)\n",
    "\n",
    "np.save('imputed_data/imputed_X_train_np_dilate.npy', imputed_X_train_np_dilate)\n",
    "np.save('imputed_data/imputed_X_train_np_restrict.npy', imputed_X_train_np_restrict)\n",
    "np.save('imputed_data/imputed_X_train_np.npy', imputed_X_train_np)\n",
    "np.save('imputed_data/imputed_X_val_np.npy', imputed_X_val_np)\n",
    "np.save('imputed_data/imputed_X_test_np.npy', imputed_X_test_np)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6841890,
     "sourceId": 10992105,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7148771,
     "sourceId": 11602930,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2322.668988,
   "end_time": "2025-04-29T08:11:56.039076",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-29T07:33:13.370088",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
