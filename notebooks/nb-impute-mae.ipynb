{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a32493d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T10:06:53.312376Z",
     "iopub.status.busy": "2025-04-20T10:06:53.312036Z",
     "iopub.status.idle": "2025-04-20T10:07:17.081259Z",
     "shell.execute_reply": "2025-04-20T10:07:17.080298Z"
    },
    "papermill": {
     "duration": 23.778096,
     "end_time": "2025-04-20T10:07:17.082978",
     "exception": false,
     "start_time": "2025-04-20T10:06:53.304882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import sys\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def manual_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # if you are suing GPU\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "manual_seed(SEED)\n",
    "\n",
    "\n",
    "# Kiểm tra và cài đặt contextily nếu chưa có\n",
    "try:\n",
    "    import contextily as ctx\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"contextily\"])\n",
    "    import contextily as ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749d877e",
   "metadata": {
    "papermill": {
     "duration": 0.005243,
     "end_time": "2025-04-20T10:07:17.094672",
     "exception": false,
     "start_time": "2025-04-20T10:07:17.089429",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Data & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5b1129",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T10:07:17.106105Z",
     "iopub.status.busy": "2025-04-20T10:07:17.105634Z",
     "iopub.status.idle": "2025-04-20T10:07:17.238600Z",
     "shell.execute_reply": "2025-04-20T10:07:17.237683Z"
    },
    "papermill": {
     "duration": 0.140054,
     "end_time": "2025-04-20T10:07:17.239937",
     "exception": false,
     "start_time": "2025-04-20T10:07:17.099883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../input/btlaionkk/data_onkk_merged.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b3d37c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T10:07:17.252622Z",
     "iopub.status.busy": "2025-04-20T10:07:17.252302Z",
     "iopub.status.idle": "2025-04-20T10:07:17.272084Z",
     "shell.execute_reply": "2025-04-20T10:07:17.271255Z"
    },
    "papermill": {
     "duration": 0.027622,
     "end_time": "2025-04-20T10:07:17.273423",
     "exception": false,
     "start_time": "2025-04-20T10:07:17.245801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess_data(df):\n",
    "    df = df.copy()\n",
    "    full_dates = pd.date_range(start=df['time'].min(), end=df['time'].max(), freq='D')\n",
    "    station_data_list = []\n",
    "    for station_id in tqdm(df.ID.unique()):\n",
    "        station_data = df[df.ID == station_id].copy()\n",
    "        if len(station_data) == 0: continue\n",
    "        station_data['time']=pd.to_datetime(station_data['time'])\n",
    "        station_data_daily = station_data.set_index('time').reindex(full_dates).rename_axis('time').reset_index()\n",
    "\n",
    "        ### Preprocess time-dependent features\n",
    "        station_data_daily['pm25_lag1'] = station_data_daily.pm25 - station_data_daily.pm25.shift(1)\n",
    "        \n",
    "        station_data_daily['lat'] = np.nanmean(station_data_daily['lat'].values)\n",
    "        station_data_daily['lon'] = np.nanmean(station_data_daily['lon'].values)\n",
    "        station_data_daily['ID'] = np.nanmean(station_data_daily['ID'].values)\n",
    "\n",
    "        ### Gather station data\n",
    "        station_data_list += [station_data_daily]\n",
    "\n",
    "    df = pd.concat(station_data_list, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    ### Preprocess time-independent features\n",
    "    df['WDIR_x'] = np.cos(np.radians(df['WDIR']))\n",
    "    df['WDIR_y'] = np.sin(np.radians(df['WDIR']))\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df[\"day_of_year\"] = df[\"time\"].dt.dayofyear\n",
    "    df[\"sin_day\"] = np.sin(2 * np.pi * df[\"day_of_year\"] / 365)\n",
    "    df[\"cos_day\"] = np.cos(2 * np.pi * df[\"day_of_year\"] / 365)\n",
    "    df['wind_u'] = df['WSPD'] * np.cos(np.radians(df['WDIR']))\n",
    "    df['wind_v'] = df['WSPD'] * np.sin(np.radians(df['WDIR']))\n",
    "    df['temp_range'] = df['TX'] - df['TN']\n",
    "    \n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df['day_of_week'] = df['time'].dt.dayofweek\n",
    "    df['month'] = df['time'].dt.month\n",
    "    \n",
    "    def get_season(month):\n",
    "        if month in [12, 1, 2]:\n",
    "            return '4'\n",
    "        elif month in [3, 4, 5]:\n",
    "            return '1'\n",
    "        elif month in [6, 7, 8]:\n",
    "            return '2'\n",
    "        elif month in [9, 10, 11]:\n",
    "            return '3'\n",
    "            \n",
    "    df['season'] = df['month'].apply(get_season).astype(int)\n",
    "    df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    df['heat_index'] = df['TMP'] * df['RH']\n",
    "    \n",
    "    def calculate_dew_point(temp, rh):\n",
    "        a = 17.27\n",
    "        b = 237.7\n",
    "        gamma = np.log(rh / 100.0) + (a * temp) / (b + temp)\n",
    "        dew_point = (b * gamma) / (a - gamma)\n",
    "        return dew_point\n",
    "        \n",
    "    df['dew_point'] = df.apply(lambda row: calculate_dew_point(row['TMP'], row['RH']), axis=1)\n",
    "    \n",
    "    hanoi_lat, hanoi_lon = 21.0278, 105.8342\n",
    "    def haversine_distance(row, lat2, lon2):\n",
    "        lat1, lon1 = row['lat'], row['lon']\n",
    "        lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "        \n",
    "        dlat = lat2 - lat1 \n",
    "        dlon = lon2 - lon1 \n",
    "        a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "        c = 2 * math.asin(math.sqrt(a))\n",
    "        r = 6371 \n",
    "        return c * r\n",
    "    \n",
    "    df['distance_to_hanoi'] = df.apply(lambda row: haversine_distance(row, hanoi_lat, hanoi_lon), axis=1)\n",
    "    \n",
    "    # df['inversion_strength'] = df['TX'] - df['TN']\n",
    "    df['temp_wind'] = df['TMP'] * df['WSPD']\n",
    "    df['rh_pressure'] = df['RH'] * df['PRES2M']\n",
    "    df['wspd_squared']= df['WSPD'] ** 2\n",
    "\n",
    "    full_dates = pd.date_range(start=df['time'].min(), end=df['time'].max(), freq='D')\n",
    "    \n",
    "    df = df.copy()\n",
    "    station_data_list = []\n",
    "    for station_id in tqdm(df.ID.unique()):\n",
    "        station_data = df[df.ID == station_id].copy()\n",
    "        if len(station_data) == 0: continue\n",
    "        # full_dates = pd.date_range(start=station_data['time'].min(), end=station_data['time'].max(), freq='D')\n",
    "        station_data['time']=pd.to_datetime(station_data['time'])\n",
    "        # station_data_daily = station_data.set_index('time').reindex(full_dates).rename_axis('time').reset_index()\n",
    "        station_data_daily = station_data.set_index('time').rename_axis('time').reset_index()\n",
    "\n",
    "        ### Preprocess time-dependent features\n",
    "        for ft_name in station_data_daily.columns:\n",
    "            if ft_name not in [\n",
    "                'CO_column_number_density', 'Cloud', 'NO2_column_number_density',\n",
    "                'O3_column_number_density', 'absorbing_aerosol_index',\n",
    "            ]: continue\n",
    "            if station_data_daily[ft_name].dtype not in ['float', 'int']:  continue\n",
    "            if ft_name in ['pm25', 'lat', 'lon', 'time', 'ID']:  continue\n",
    "            station_data_daily[f'{ft_name}_prev1'] = station_data_daily[ft_name].shift(1)\n",
    "            station_data_daily[f'{ft_name}_next1'] = station_data_daily[ft_name].shift(-1)\n",
    "\n",
    "        ### Gather station data\n",
    "        station_data_list += [station_data_daily]\n",
    "\n",
    "    df = pd.concat(station_data_list, axis=0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_timeseries_data(df, window_size=16, show_tqdm=True):\n",
    "    ### Assume these 2 vars have been setup\n",
    "    global features, scaler\n",
    "\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    pbar = df.ID.unique()\n",
    "    if show_tqdm: pbar = tqdm(pbar)\n",
    "    for station_id in pbar:\n",
    "        station_data = df[df.ID == station_id].copy()\n",
    "        if len(station_data) == 0: continue\n",
    "        station_data['time']=pd.to_datetime(station_data['time'])\n",
    "        full_dates = pd.date_range(start=station_data['time'].min(), end=station_data['time'].max(), freq='D')\n",
    "        station_data_daily = station_data.set_index('time').reindex(full_dates).rename_axis('time').reset_index()\n",
    "    \n",
    "        for i in range(len(station_data_daily) - window_size):\n",
    "            # Do scaling stuff\n",
    "            currX = scaler.transform(station_data_daily[features].iloc[i:i+window_size].values)\n",
    "            curry = scaler.transform(station_data_daily[features].iloc[[i+window_size]].values)[0]\n",
    "\n",
    "            # Let the pm25 be in the first column\n",
    "            pm25_idx = features.index('pm25')\n",
    "            curry = curry[[pm25_idx] + [i for i in range(len(features)) if i != pm25_idx]]\n",
    "            \n",
    "            if np.isnan(np.sum(currX)) or np.isnan(np.sum(curry)):\n",
    "                continue\n",
    "            Xs += [currX]\n",
    "            ys += [curry]\n",
    "\n",
    "    X = np.stack(Xs)\n",
    "    y = np.stack(ys)\n",
    "    return X, y\n",
    "\n",
    "def create_timeseries_data_missing(df, window_size=8, show_tqdm=True):\n",
    "    ### Assume these 2 vars have been setup\n",
    "    global features, scaler\n",
    "    full_dates = pd.date_range(start=df['time'].min(), end=df['time'].max(), freq='D')\n",
    "\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    pbar = data.ID.unique()\n",
    "    if show_tqdm: pbar = tqdm(pbar)\n",
    "    for station_id in pbar:\n",
    "        station_data = df[df.ID == station_id].copy()\n",
    "        if len(station_data) == 0: continue\n",
    "        station_data['time']=pd.to_datetime(station_data['time'])\n",
    "        # station_data_daily = station_data.set_index('time').reindex(full_dates).rename_axis('time').reset_index()\n",
    "        station_data_daily = station_data.set_index('time').rename_axis('time').reset_index()\n",
    "\n",
    "        \n",
    "        currX = scaler.transform(station_data_daily[features].values)\n",
    "        # currX[:, 1] = np.nanmean(currX[:, 1])\n",
    "        # currX[:, 2] = np.nanmean(currX[:, 2])\n",
    "        Xs += [currX[:, None]]\n",
    "    Xs = np.concatenate(Xs, axis=1)\n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9452c5f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T10:07:17.285063Z",
     "iopub.status.busy": "2025-04-20T10:07:17.284834Z",
     "iopub.status.idle": "2025-04-20T10:07:18.269384Z",
     "shell.execute_reply": "2025-04-20T10:07:18.268482Z"
    },
    "papermill": {
     "duration": 0.991928,
     "end_time": "2025-04-20T10:07:18.270822",
     "exception": false,
     "start_time": "2025-04-20T10:07:17.278894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_processed = preprocess_data(data)\n",
    "data_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abbc799",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T10:07:18.283832Z",
     "iopub.status.busy": "2025-04-20T10:07:18.283602Z",
     "iopub.status.idle": "2025-04-20T10:07:18.292564Z",
     "shell.execute_reply": "2025-04-20T10:07:18.291750Z"
    },
    "papermill": {
     "duration": 0.01674,
     "end_time": "2025-04-20T10:07:18.293882",
     "exception": false,
     "start_time": "2025-04-20T10:07:18.277142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_date = '2021-06-01'\n",
    "test_date = '2021-08-01'\n",
    "train = data_processed[data_processed['time'] < val_date]\n",
    "val = data_processed[(data_processed['time'] >= val_date) & (data_processed['time'] < test_date)]\n",
    "test = data_processed[data_processed['time'] >= test_date]\n",
    "#train = train.drop('ID',axis=1)\n",
    "#train = train.drop('time',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eef571",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T10:07:18.306587Z",
     "iopub.status.busy": "2025-04-20T10:07:18.306345Z",
     "iopub.status.idle": "2025-04-20T10:07:18.310970Z",
     "shell.execute_reply": "2025-04-20T10:07:18.310151Z"
    },
    "papermill": {
     "duration": 0.012283,
     "end_time": "2025-04-20T10:07:18.312112",
     "exception": false,
     "start_time": "2025-04-20T10:07:18.299829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_processed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afc75c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T10:07:18.324991Z",
     "iopub.status.busy": "2025-04-20T10:07:18.324756Z",
     "iopub.status.idle": "2025-04-20T10:07:18.339036Z",
     "shell.execute_reply": "2025-04-20T10:07:18.338412Z"
    },
    "papermill": {
     "duration": 0.022184,
     "end_time": "2025-04-20T10:07:18.340315",
     "exception": false,
     "start_time": "2025-04-20T10:07:18.318131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    ### Features selection\n",
    "    # 'time', 'ID',\n",
    "    'pm25',\n",
    "    'lat', 'lon',\n",
    "    'sin_day', 'cos_day',\n",
    "    'SQRT_SEA_DEM_LAT', 'WSPD',\n",
    "    'WDIR',\n",
    "    'TMP',\n",
    "    'TX', 'TN', 'TP', 'RH', 'PRES2M',\n",
    "    # 'pm25_lag1',\n",
    "    # 'WDIR_x', 'WDIR_y',\n",
    "    # 'day_of_year',\n",
    "    'wind_u', 'wind_v',\n",
    "    # 'temp_range',\n",
    "    # 'day_of_week', 'month', 'season', 'is_weekend',\n",
    "    'heat_index', 'dew_point', 'distance_to_hanoi', 'temp_wind',\n",
    "    'rh_pressure',\n",
    "    # 'wspd_squared',\n",
    "    'CO_column_number_density', 'Cloud', 'NO2_column_number_density',\n",
    "    'O3_column_number_density', 'absorbing_aerosol_index',\n",
    "]\n",
    "\n",
    "### Fit a shared scaler on training data\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e5dc36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T10:07:18.353872Z",
     "iopub.status.busy": "2025-04-20T10:07:18.353641Z",
     "iopub.status.idle": "2025-04-20T10:07:18.358490Z",
     "shell.execute_reply": "2025-04-20T10:07:18.357697Z"
    },
    "papermill": {
     "duration": 0.013066,
     "end_time": "2025-04-20T10:07:18.359756",
     "exception": false,
     "start_time": "2025-04-20T10:07:18.346690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    \"pm25\",\n",
    "    \n",
    "    \"SQRT_SEA_DEM_LAT\",\n",
    "    \n",
    "    \"TN\", \"dew_point\", \"heat_index\", \"TMP\", \"sin_day\", \"PRES2M\",\n",
    "    \"distance_to_hanoi\", \"temp_wind\", \"cos_day\", \"TP\", \"TX\", \"wind_u\",\n",
    "    \"rh_pressure\",\n",
    "\n",
    "    'CO_column_number_density', 'Cloud', 'NO2_column_number_density',\n",
    "    'O3_column_number_density', 'absorbing_aerosol_index',\n",
    "]\n",
    "\n",
    "print([ft for ft in features if ft not in selected_features])\n",
    "print([ft for ft in selected_features if ft not in features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34f97ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T10:07:18.373284Z",
     "iopub.status.busy": "2025-04-20T10:07:18.373050Z",
     "iopub.status.idle": "2025-04-20T10:07:18.378121Z",
     "shell.execute_reply": "2025-04-20T10:07:18.377316Z"
    },
    "papermill": {
     "duration": 0.0133,
     "end_time": "2025-04-20T10:07:18.379248",
     "exception": false,
     "start_time": "2025-04-20T10:07:18.365948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos_embed_indices = [1,2,3,4]\n",
    "np.array(features)[pos_embed_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d99200",
   "metadata": {
    "papermill": {
     "duration": 0.006214,
     "end_time": "2025-04-20T10:07:18.391837",
     "exception": false,
     "start_time": "2025-04-20T10:07:18.385623",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308c9a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T10:07:18.405301Z",
     "iopub.status.busy": "2025-04-20T10:07:18.405068Z",
     "iopub.status.idle": "2025-04-20T10:07:18.495817Z",
     "shell.execute_reply": "2025-04-20T10:07:18.495120Z"
    },
    "papermill": {
     "duration": 0.099042,
     "end_time": "2025-04-20T10:07:18.497125",
     "exception": false,
     "start_time": "2025-04-20T10:07:18.398083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_np = create_timeseries_data_missing(train, show_tqdm=False)\n",
    "station_locs = X_train_np[0, :, [1, 2]]\n",
    "import numpy as np\n",
    "\n",
    "coords = station_locs.T   # shape → (26, 2)\n",
    "\n",
    "diff    = coords[:, np.newaxis, :] - coords[np.newaxis, :, :]   # → (26,26,2)\n",
    "sqdist  = np.sum(diff**2, axis=2)                               # → (26,26)\n",
    "\n",
    "np.fill_diagonal(sqdist, np.inf)\n",
    "\n",
    "nearest_stations = np.argsort(sqdist, axis=1)[:, :9]  # shape → (26,4)\n",
    "\n",
    "nearest_stations = nearest_stations.tolist()\n",
    "stations_sampling = [[i] + nearest_stations[i] for i in range(26)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3bef42",
   "metadata": {
    "papermill": {
     "duration": 0.006138,
     "end_time": "2025-04-20T10:07:18.509900",
     "exception": false,
     "start_time": "2025-04-20T10:07:18.503762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579fbcb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T10:07:18.523405Z",
     "iopub.status.busy": "2025-04-20T10:07:18.523125Z",
     "iopub.status.idle": "2025-04-20T10:07:18.533548Z",
     "shell.execute_reply": "2025-04-20T10:07:18.532922Z"
    },
    "papermill": {
     "duration": 0.018475,
     "end_time": "2025-04-20T10:07:18.534788",
     "exception": false,
     "start_time": "2025-04-20T10:07:18.516313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df, window_sizes=[16, 32, 64], batch_size=16):\n",
    "        \"\"\"\n",
    "        Creates a dataset that precomputes timeseries data for multiple window sizes.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): The input dataframe.\n",
    "            window_sizes (list): A list of integers specifying window sizes.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.window_sizes = window_sizes\n",
    "        self.batch_size = batch_size\n",
    "        self.data_dict = {}\n",
    "        \n",
    "        # Precompute timeseries data for each window size and store in a dictionary.\n",
    "        for ws in tqdm(window_sizes):\n",
    "            X, y = create_timeseries_data(df, window_size=ws, show_tqdm=False)\n",
    "            self.data_dict[ws] = {'X': X, 'y': y}\n",
    "\n",
    "    def __len__(self):\n",
    "        # Define length as the sum of all samples computed for all window sizes.\n",
    "        total = 0\n",
    "        for ws in self.window_sizes:\n",
    "            total += len(self.data_dict[ws]['X'])\n",
    "        return total // self.batch_size\n",
    "\n",
    "    def __getitem__(self, dummy_index):\n",
    "        if dummy_index > self.__len__(): raise StopIteration\n",
    "        ws = random.choice(self.window_sizes)\n",
    "        data = self.data_dict[ws]\n",
    "        # Randomly sample an index from the chosen data.\n",
    "        sample_idx = random.sample(range(len(data['X'])), k=self.batch_size)\n",
    "        sample = {\n",
    "            'X': data['X'][sample_idx],\n",
    "            'y': data['y'][sample_idx],\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "\n",
    "class ImputeDataset(Dataset):\n",
    "    def __init__(self, df, time_window_size=7, station_window_size=5, sampling_size=10, additional_mask_probs=0.3):\n",
    "        \"\"\"\n",
    "        Creates a dataset that precomputes timeseries data for multiple window sizes.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): The input dataframe.\n",
    "            window_sizes (list): A list of integers specifying window sizes.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "\n",
    "        self.time_window_size=time_window_size\n",
    "        self.station_window_size=station_window_size\n",
    "        \n",
    "        self.X = create_timeseries_data_missing(df, show_tqdm=False)\n",
    "        self.mask = ~np.isnan(self.X)\n",
    "        self.additional_mask = (np.random.rand(*self.mask.shape) > additional_mask_probs) & self.mask\n",
    "        self.X[~self.mask] = 0.0\n",
    "\n",
    "        self.sample_indices = []\n",
    "\n",
    "        self.sampling_size = sampling_size\n",
    "\n",
    "        self._prepare()\n",
    "\n",
    "    def _prepare(self, ):\n",
    "        T, S, N = self.X.shape\n",
    "        for i in range(T):\n",
    "            if i + self.time_window_size >= T: break\n",
    "            time_indices = np.arange(i, i + self.time_window_size)\n",
    "            for _ in range(self.sampling_size):\n",
    "                station_indices = random.sample(\n",
    "                    random.choice(stations_sampling), k=self.station_window_size)\n",
    "                mask_batch = self.mask[time_indices][:, station_indices]\n",
    "                if mask_batch.mean() < 0.5: continue\n",
    "                self.sample_indices.append([time_indices, station_indices])\n",
    "                \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_indices)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        time_indices, station_indices = self.sample_indices[i]\n",
    "        batch = torch.tensor(\n",
    "            self.X[time_indices][:, station_indices], dtype=torch.float32)\n",
    "        mask_batch = torch.tensor(\n",
    "            self.mask[time_indices][:, station_indices], dtype=torch.float32)\n",
    "        additional_mask_batch = torch.tensor(\n",
    "            self.additional_mask[time_indices][:, station_indices], dtype=torch.float32)\n",
    "        \n",
    "        return (batch, mask_batch, additional_mask_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f81a09e",
   "metadata": {
    "papermill": {
     "duration": 0.006238,
     "end_time": "2025-04-20T10:07:18.547389",
     "exception": false,
     "start_time": "2025-04-20T10:07:18.541151",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb5bd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T10:07:18.560958Z",
     "iopub.status.busy": "2025-04-20T10:07:18.560726Z",
     "iopub.status.idle": "2025-04-20T10:07:18.575164Z",
     "shell.execute_reply": "2025-04-20T10:07:18.574605Z"
    },
    "papermill": {
     "duration": 0.022813,
     "end_time": "2025-04-20T10:07:18.576411",
     "exception": false,
     "start_time": "2025-04-20T10:07:18.553598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../models/')\n",
    "\n",
    "from mae import MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f8358c",
   "metadata": {},
   "source": [
    "# Training Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3168f7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T10:07:18.590978Z",
     "iopub.status.busy": "2025-04-20T10:07:18.590743Z",
     "iopub.status.idle": "2025-04-20T10:07:18.599985Z",
     "shell.execute_reply": "2025-04-20T10:07:18.599172Z"
    },
    "papermill": {
     "duration": 0.017324,
     "end_time": "2025-04-20T10:07:18.601206",
     "exception": false,
     "start_time": "2025-04-20T10:07:18.583882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "from copy import deepcopy\n",
    "import time\n",
    "def eval_model(model, data_loader):\n",
    "    model = deepcopy(model).eval().to(args.device)\n",
    "    tick = time.time()\n",
    "    losses = []\n",
    "    for sample in data_loader:\n",
    "        batch, mask_batch, additional_mask_batch = sample\n",
    "        batch = batch[0].to(args.device)\n",
    "        mask_batch = mask_batch[0].to(args.device)\n",
    "        additional_mask_batch = additional_mask_batch[0].to(args.device)\n",
    "\n",
    "        rec_mask = mask_batch - additional_mask_batch\n",
    "        \n",
    "        data_pred = model.forward_impute(batch, additional_mask_batch)\n",
    "        loss = ((data_pred - batch) ** 2 * rec_mask).sum() / (rec_mask.sum() + 1e-8)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    return np.mean(losses)\n",
    "\n",
    "def train_model(model, train_loader, val_loader):\n",
    "    torch.cuda.empty_cache()\n",
    "    manual_seed(SEED)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    prev_eval_score = -9999.0\n",
    "    best_eval_score = -9999.0\n",
    "    iteration = 0\n",
    "    mean_loss = -1\n",
    "    best_ckpt_state_dict = deepcopy(model.state_dict())\n",
    "\n",
    "    model.train().to(args.device)\n",
    "    for epoch in range(args.num_epochs):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        pbar = tqdm(train_loader, position=0, leave=True)\n",
    "        for sample in pbar:\n",
    "            batch, mask_batch, additional_mask_batch = sample\n",
    "            batch = batch[0].to(args.device)\n",
    "            mask_batch = mask_batch[0].to(args.device)\n",
    "            additional_mask_batch = additional_mask_batch[0].to(args.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.forward_ssl(batch, mask_batch, mask_ratio=args.mask_ratio)\n",
    "            loss.backward()\n",
    "\n",
    "            total_norm = 0\n",
    "            for p in model.parameters():\n",
    "                try:\n",
    "                    param_norm = p.grad.detach().data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "                except: pass\n",
    "            total_norm = total_norm ** 0.5\n",
    "            optimizer.step()\n",
    "            \n",
    "            if mean_loss is None or mean_loss < 0: mean_loss = loss.item()\n",
    "            else: mean_loss = 0.9 * mean_loss + 0.1 * loss.item()\n",
    "\n",
    "\n",
    "                \n",
    "            if (iteration + 1) % 20 == 0:\n",
    "                if best_eval_score > -9999:\n",
    "                    pbar.set_description_str(\n",
    "                    f\"Loss: {mean_loss:.4f} | Best Val Score: {best_eval_score:.4f} | Val Score: {prev_eval_score:.4f}\\t\")\n",
    "                else:\n",
    "                    pbar.set_description_str(\n",
    "                    f\"Loss: {mean_loss:.4f}\\t\")\n",
    "            if (iteration + 1) % 500 == 0:\n",
    "                # Evaluate on test data.\n",
    "                eval_score = -eval_model(model, val_loader)\n",
    "                prev_eval_score = eval_score\n",
    "                if eval_score > best_eval_score:\n",
    "                    best_eval_score = eval_score\n",
    "                    best_ckpt_state_dict = deepcopy(model.state_dict())\n",
    "                    torch.save(model.state_dict(), 'best_model_pretrain.pt')\n",
    "            iteration += 1\n",
    "    model.load_state_dict(best_ckpt_state_dict)\n",
    "    return model, best_eval_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a5db6f",
   "metadata": {
    "papermill": {
     "duration": 0.006324,
     "end_time": "2025-04-20T10:07:18.613659",
     "exception": false,
     "start_time": "2025-04-20T10:07:18.607335",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f61c32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T10:07:18.626786Z",
     "iopub.status.busy": "2025-04-20T10:07:18.626582Z",
     "iopub.status.idle": "2025-04-20T10:07:18.683082Z",
     "shell.execute_reply": "2025-04-20T10:07:18.682432Z"
    },
    "papermill": {
     "duration": 0.064649,
     "end_time": "2025-04-20T10:07:18.684470",
     "exception": false,
     "start_time": "2025-04-20T10:07:18.619821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import numpy as np\n",
    "\n",
    "# args = Namespace(\n",
    "#     device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "#     seed=1902,\n",
    "#     ft_embed_dim=8,\n",
    "#     ft_enc_nhead=1,\n",
    "#     ft_enc_num_layers=3,\n",
    "#     mae_hidden_dim=64,\n",
    "#     mlp_ratio=4.,\n",
    "#     dim_feedforward=64 * 4,\n",
    "#     mae_nhead=4,\n",
    "#     mae_num_layers=3,\n",
    "#     mae_dropout=0.1,\n",
    "#     activation='gelu',\n",
    "#     time_window_size=5,\n",
    "#     station_window_size=5,\n",
    "#     num_epochs=2,\n",
    "#     lr=3e-4,\n",
    "#     weight_decay=1e-6,\n",
    "#     mask_ratio=0.3,\n",
    "# )\n",
    "\n",
    "args = Namespace( # Optuna\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    seed=1902,\n",
    "    ft_embed_dim=24,\n",
    "    ft_enc_nhead=2,\n",
    "    ft_enc_num_layers=4,\n",
    "    mae_hidden_dim=160,\n",
    "    mlp_ratio=4.36,\n",
    "    dim_feedforward=697,\n",
    "    mae_nhead=4,\n",
    "    mae_num_layers=3,\n",
    "    mae_dropout=0.29,\n",
    "    activation='gelu',\n",
    "    time_window_size=5,\n",
    "    station_window_size=5,\n",
    "    num_epochs=2,\n",
    "    lr=6.9e-5,\n",
    "    weight_decay=4.12e-6,\n",
    "    mask_ratio=0.31,\n",
    ")\n",
    "\n",
    "args.pos_embed_indices = pos_embed_indices\n",
    "args.num_features = len(features)\n",
    "\n",
    "def manual_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # if you are using GPU\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05efcc1c",
   "metadata": {
    "papermill": {
     "duration": 0.006205,
     "end_time": "2025-04-20T10:07:18.697385",
     "exception": false,
     "start_time": "2025-04-20T10:07:18.691180",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Setup datasets and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce59eb09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T10:07:18.710809Z",
     "iopub.status.busy": "2025-04-20T10:07:18.710580Z",
     "iopub.status.idle": "2025-04-20T10:07:21.038410Z",
     "shell.execute_reply": "2025-04-20T10:07:21.037669Z"
    },
    "papermill": {
     "duration": 2.336294,
     "end_time": "2025-04-20T10:07:21.040021",
     "exception": false,
     "start_time": "2025-04-20T10:07:18.703727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = ImputeDataset(train, args.time_window_size, args.station_window_size,\n",
    "                              sampling_size=200, additional_mask_probs=0.0)\n",
    "val_dataset = ImputeDataset(val, args.time_window_size, args.station_window_size,\n",
    "                            sampling_size=20, additional_mask_probs=0.3)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fd1310",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T10:07:21.053920Z",
     "iopub.status.busy": "2025-04-20T10:07:21.053692Z",
     "iopub.status.idle": "2025-04-20T12:09:07.526032Z",
     "shell.execute_reply": "2025-04-20T12:09:07.524964Z"
    },
    "papermill": {
     "duration": 7306.480701,
     "end_time": "2025-04-20T12:09:07.527488",
     "exception": false,
     "start_time": "2025-04-20T10:07:21.046787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae = MAE(args)\n",
    "mae, best_eval_score = train_model(mae, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b22b55d",
   "metadata": {},
   "source": [
    "# Load model and create imputed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3643be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = deepcopy(mae).eval().to(args.device)\n",
    "try:best_model.load_state_dict(torch.load('./best_model.pt'))\n",
    "except:\n",
    "    try: best_model.load_state_dict(torch.load('./best_model_pretrain.pt'))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc78c7d0",
   "metadata": {},
   "source": [
    "### Calculate nearest stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199723f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = create_timeseries_data_missing(train, show_tqdm=False)\n",
    "station_locs = X_train_np[0, :, [1, 2]]\n",
    "import numpy as np\n",
    "\n",
    "coords = station_locs.T   # shape → (26, 2)\n",
    "\n",
    "diff    = coords[:, np.newaxis, :] - coords[np.newaxis, :, :]   # → (26,26,2)\n",
    "sqdist  = np.sum(diff**2, axis=2)                               # → (26,26)\n",
    "\n",
    "np.fill_diagonal(sqdist, np.inf)\n",
    "\n",
    "nearest_stations = np.argsort(sqdist, axis=1)[:, :args.station_window_size - 1]  # shape → (26,4)\n",
    "\n",
    "nearest_stations = nearest_stations.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea6a82e",
   "metadata": {},
   "source": [
    "### 📌 Imputation Rules\n",
    "\n",
    "To ensure the best use of incomplete data while maintaining strict evaluation integrity, we apply different imputation strategies for training, validation, and test sets. These strategies aim to **(1) reduce reconstruction error during training** and **(2) prevent information leakage during evaluation**.\n",
    "\n",
    "#### 🔧 Training Set\n",
    "For the training set, our goal is to provide the downstream LSTM model with the most accurate reconstructed values. Therefore, **future data is allowed** during imputation to maximize the information available. Each missing value is filled using predictions from a masked autoencoder (MAE) applied on a spatiotemporal window that includes the target station and its nearest neighbors. The MAE is trained and selected based on validation MSE.\n",
    "\n",
    "We create multiple versions of the imputed training data:\n",
    "- `imputed_X_train_np`: Fully imputed version for inspection or optional use.\n",
    "- `imputed_X_train_np_dilate`: Only retains values that are supported by sufficiently reliable neighboring information.\n",
    "- `imputed_X_train_np_restrict`: Masks out values that were originally missing in the PM2.5 feature.\n",
    "\n",
    "The `imputed_X_train_np_dilate` version selectively retains imputed values based on local spatiotemporal support. For each station, we apply 1D `binary_dilation` over time to extend valid PM2.5 observations, then merge this mask with that of the station’s nearest neighbor. This ensures a value is only kept if the original data had support either at that station or nearby in time/space. All other imputed values are discarded by resetting them to `NaN`.\n",
    "\n",
    "#### 🧪 Validation & Test Sets\n",
    "In contrast to training, **future data is strictly prohibited** during imputation on validation and test sets to prevent any leakage. Each sample is imputed **only using past and current data**, ensuring that the evaluation remains unbiased.\n",
    "\n",
    "For each time point `t`, only the time window `[t - W + 1, t]` is used (where `W` is the window size). The MAE model imputes missing values in this window using the most informative set of stations — consisting of the target station and its closest neighbors — as determined dynamically by mask coverage.\n",
    "\n",
    "Only the **last time step in each window is retained** after imputation, corresponding to the target input of the LSTM predictor. Furthermore, any validation/test sample where the target PM2.5 label is missing is **excluded from downstream evaluation** to maintain fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53400b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "           \n",
    "X_train_np = create_timeseries_data_missing(train, show_tqdm=False)\n",
    "mask_np = ~np.isnan(X_train_np)\n",
    "\n",
    "imputed_X_train_np = np.zeros_like(X_train_np) + np.nan\n",
    "mask_blocks = mask_np.mean(axis=-1) < 0.5\n",
    "mask_blocks.shape\n",
    "\n",
    "T, S, N = X_train_np.shape\n",
    "for t in tqdm(range(T)):\n",
    "    if t + args.time_window_size >= T: break\n",
    "    time_indices = np.arange(t, t + args.time_window_size)\n",
    "    for s in range(S):\n",
    "        if mask_blocks[t].mean() < 0.1: continue\n",
    "        best_station_indices = None\n",
    "        best_score = 0.0\n",
    "        station_indices = [s] + nearest_stations[s]\n",
    "        mask_batch = mask_np[time_indices][:, station_indices]\n",
    "        if mask_batch.mean() > best_score:\n",
    "            best_score = mask_batch.mean()\n",
    "            best_station_indices = deepcopy(station_indices)\n",
    "        if best_score < 0.5: \n",
    "            continue\n",
    "        station_indices = best_station_indices\n",
    "        batch = torch.tensor(\n",
    "            X_train_np[time_indices][:, station_indices], dtype=torch.float32, device=args.device)\n",
    "        batch = batch.nan_to_num(0.0)\n",
    "        mask_batch = torch.tensor(\n",
    "            mask_np[time_indices][:, station_indices], dtype=torch.float32, device=args.device)\n",
    "        with torch.no_grad():\n",
    "            data_pred = best_model.forward_impute(batch, mask_batch)\n",
    "            data_pred = data_pred * (1 - mask_batch) + batch * mask_batch\n",
    "        data_pred = data_pred.detach().cpu().numpy()\n",
    "        imputed_X_train_np[time_indices, s] = data_pred[:, 0]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2d9e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "           \n",
    "X_val_np = create_timeseries_data_missing(val, show_tqdm=False)\n",
    "mask_np = ~np.isnan(X_val_np)\n",
    "\n",
    "imputed_X_val_np = np.zeros_like(X_val_np) + np.nan\n",
    "mask_blocks = mask_np.mean(axis=-1) < 0.5\n",
    "mask_blocks.shape\n",
    "\n",
    "T, S, N = X_val_np.shape\n",
    "for t_reverse in tqdm(range(args.time_window_size - 1, T)):\n",
    "    t = t_reverse - args.time_window_size + 1\n",
    "    if t < 0: break\n",
    "    time_indices = np.arange(t, t + args.time_window_size)\n",
    "    for s in range(S):\n",
    "        if mask_blocks[t].mean() < 0.1: continue\n",
    "        best_station_indices = None\n",
    "        best_score = 0.0\n",
    "        station_indices = [s] + nearest_stations[s]\n",
    "        mask_batch = mask_np[time_indices][:, station_indices]\n",
    "        if mask_batch.mean() > best_score:\n",
    "            best_score = mask_batch.mean()\n",
    "            best_station_indices = deepcopy(station_indices)\n",
    "        if best_score < 0.5: \n",
    "            continue\n",
    "        station_indices = best_station_indices\n",
    "        batch = torch.tensor(\n",
    "            X_val_np[time_indices][:, station_indices], dtype=torch.float32, device=args.device)\n",
    "        batch = batch.nan_to_num(0.0)\n",
    "        mask_batch = torch.tensor(\n",
    "            mask_np[time_indices][:, station_indices], dtype=torch.float32, device=args.device)\n",
    "        with torch.no_grad():\n",
    "            data_pred = best_model.forward_impute(batch, mask_batch)\n",
    "            data_pred = data_pred * (1 - mask_batch) + batch * mask_batch\n",
    "        data_pred = data_pred.detach().cpu().numpy()\n",
    "        imputed_X_val_np[time_indices[-1], s] = data_pred[-1, 0]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05adee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test_np = create_timeseries_data_missing(test, show_tqdm=False)\n",
    "mask_np = ~np.isnan(X_test_np)\n",
    "\n",
    "imputed_X_test_np = np.zeros_like(X_test_np) + np.nan\n",
    "mask_blocks = mask_np.mean(axis=-1) < 0.5\n",
    "mask_blocks.shape\n",
    "\n",
    "T, S, N = X_test_np.shape\n",
    "for t_reverse in tqdm(range(args.time_window_size - 1, T)):\n",
    "    t = t_reverse - args.time_window_size + 1\n",
    "    if t < 0: break\n",
    "    time_indices = np.arange(t, t + args.time_window_size)\n",
    "    for s in range(S):\n",
    "        if mask_blocks[t].mean() < 0.1: continue\n",
    "        best_station_indices = None\n",
    "        best_score = 0.0\n",
    "        station_indices = [s] + nearest_stations[s]\n",
    "        mask_batch = mask_np[time_indices][:, station_indices]\n",
    "        if mask_batch.mean() > best_score:\n",
    "            best_score = mask_batch.mean()\n",
    "            best_station_indices = deepcopy(station_indices)\n",
    "        if best_score < 0.5: \n",
    "            continue\n",
    "        station_indices = best_station_indices\n",
    "        batch = torch.tensor(\n",
    "            X_test_np[time_indices][:, station_indices], dtype=torch.float32, device=args.device)\n",
    "        batch = batch.nan_to_num(0.0)\n",
    "        mask_batch = torch.tensor(\n",
    "            mask_np[time_indices][:, station_indices], dtype=torch.float32, device=args.device)\n",
    "        with torch.no_grad():\n",
    "            data_pred = best_model.forward_impute(batch, mask_batch)\n",
    "            data_pred = data_pred * (1 - mask_batch) + batch * mask_batch\n",
    "        data_pred = data_pred.detach().cpu().numpy()\n",
    "        imputed_X_test_np[time_indices[-1], s] = data_pred[-1, 0]\n",
    "        \n",
    "    \n",
    "imputed_X_test_np_full = imputed_X_test_np.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003cd63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.ndimage import binary_dilation\n",
    "train_keep_mask = ~np.isnan(X_train_np[:, :, 0])[:, :]\n",
    "T, S = train_keep_mask.shape\n",
    "for s in range(S):\n",
    "    train_keep_mask[:, s] = binary_dilation(train_keep_mask[:, s])\n",
    "    \n",
    "    for s1 in nearest_stations[s][:1]:\n",
    "        train_keep_mask[:, s] |= train_keep_mask[:, s1]\n",
    "\n",
    "imputed_X_train_np_dilate = imputed_X_train_np.copy()\n",
    "imputed_X_train_np_restrict = imputed_X_train_np.copy()\n",
    "\n",
    "imputed_X_train_np_dilate[~train_keep_mask] = np.nan\n",
    "imputed_X_train_np_restrict[np.isnan(X_train_np[:, :, 0])[:, :]] = np.nan\n",
    "\n",
    "imputed_X_val_np[np.isnan(X_val_np[:, :, 0])[:, :]] = np.nan\n",
    "imputed_X_test_np[np.isnan(X_test_np[:, :, 0])[:, :]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1d47ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('imputed_data', exist_ok=True)\n",
    "\n",
    "np.save('imputed_data/imputed_X_train_np_dilate.npy', imputed_X_train_np_dilate)\n",
    "np.save('imputed_data/imputed_X_train_np_restrict.npy', imputed_X_train_np_restrict)\n",
    "np.save('imputed_data/imputed_X_train_np.npy', imputed_X_train_np)\n",
    "np.save('imputed_data/imputed_X_val_np.npy', imputed_X_val_np)\n",
    "np.save('imputed_data/imputed_X_test_np.npy', imputed_X_test_np)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11926933,
     "datasetId": 7148771,
     "sourceId": 11481615,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 11370960,
     "datasetId": 6841890,
     "sourceId": 10992105,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7491.378807,
   "end_time": "2025-04-20T12:11:42.095272",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-20T10:06:50.716465",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
